# pipeline.py
import os
import json
import time
import hashlib
from typing import List, Dict, Any, Tuple, Union
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# --- 1. NVIDIA NIM CLIENT ---
class NIMClient:
    """Client ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi NVIDIA NIM qua API t∆∞∆°ng th√≠ch OpenAI."""
    def __init__(self, api_key: str = "", base_url: str = "", model: str = "meta/llama-3.1-405b-instruct"):
        api_key = api_key or os.environ.get("NIM_API_KEY", "")
        base_url = base_url or os.environ.get("NIM_BASE_URL", "https://integrate.api.nvidia.com/v1")
        if not api_key:
            print("‚ö†Ô∏è NIM_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y. Chatbot s·∫Ω ch·∫°y ·ªü ch·∫ø ƒë·ªô demo.")
            # Thay v√¨ raise error, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng m·ªôt API key demo ho·∫∑c fallback
            api_key = "demo_key"  # Placeholder - b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh logic n√†y
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.is_demo_mode = api_key == "demo_key"

    def chat(self, messages, temperature: float = 0.1, max_tokens: int = 1024, top_p: float = 0.7, **kwargs) -> str:
        """G·ª≠i y√™u c·∫ßu chat v√† nh·∫≠n ph·∫£n h·ªìi."""
        if self.is_demo_mode:
            return "Xin l·ªói, chatbot ƒëang ·ªü ch·∫ø ƒë·ªô demo do thi·∫øu c·∫•u h√¨nh API key. Vui l√≤ng li√™n h·ªá admin ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                    stream=False,
                    **kwargs
                )
                return (resp.choices[0].message.content or "").strip()
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"‚ùå API call failed after {max_retries} attempts: {e}")
                    return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n: {str(e)}"
                print(f"‚ö†Ô∏è API call failed (attempt {attempt + 1}/{max_retries}), retrying...")
                time.sleep(2 ** attempt)
        return "Xin l·ªói, kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n l√∫c n√†y."

# --- 2. EMBEDDING MODEL ---
class Embeddings:
    """T·∫£i v√† s·ª≠ d·ª•ng model embedding t·ª´ HuggingFace."""
    def __init__(self, model_path="iwillcthew/vietnamese-embedding-PROPTIT-domain-ft"):
        print(f"üöÄ Loading embedding model: {model_path}")
        self.model = SentenceTransformer(model_path)
        self.model.max_seq_length = 2048
        print("‚úÖ Embedding model loaded.")

    def encode(self, text: Union[str, List[str]]):
        """T·∫°o embedding cho vƒÉn b·∫£n."""
        return self.model.encode(text)

# --- 3. VECTOR DATABASE ---
class VectorDatabaseAtlas:
    """K·∫øt n·ªëi v√† truy v·∫•n MongoDB Atlas Vector Search."""
    def __init__(self, mongodb_uri: str = ""):
        mongodb_uri = mongodb_uri or os.environ.get("MONGODB_URI", "")
        if not mongodb_uri:
            print("‚ö†Ô∏è MONGODB_URI kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y. Vector search s·∫Ω kh√¥ng kh·∫£ d·ª•ng.")
            self.client = None
            self.db = None
            self.collection = None
            self.is_demo_mode = True
            return
        
        try:
            print("üîó Connecting to MongoDB Atlas...")
            self.client = MongoClient(mongodb_uri)
            self.db = self.client.get_database("vector_db")
            self.collection = self.db["information"]
            self.is_demo_mode = False
            print("‚úÖ MongoDB Atlas connected.")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to connect to MongoDB: {e}")
            self.client = None
            self.db = None
            self.collection = None
            self.is_demo_mode = True

    def vector_search(self, query_vector: List[float], limit: int = 30) -> List[Dict]:
        """Th·ª±c hi·ªán vector search."""
        if self.is_demo_mode or self.collection is None:
            print("‚ö†Ô∏è Vector search kh√¥ng kh·∫£ d·ª•ng trong ch·∫ø ƒë·ªô demo.")
            return []
        
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "vector_index",
                    "queryVector": query_vector,
                    "path": "embedding",
                    "numCandidates": max(200, limit * 10),
                    "limit": limit
                }
            }
        ]
        try:
            return list(self.collection.aggregate(pipeline, maxTimeMS=30000))
        except Exception as e:
            print(f"‚ö†Ô∏è Vector search failed: {e}")
            return []

# --- 4. LLM RERANKER ---
class LLMReranker:
    """S·ª≠ d·ª•ng LLM ƒë·ªÉ re-rank k·∫øt qu·∫£ t√¨m ki·∫øm."""
    def __init__(self, nim_client: NIMClient):
        self.nim = nim_client

    def rerank(self, query: str, results: List[Dict[str, Any]], top_k: int = 7) -> Tuple[List[Dict[str, Any]], List[int]]:
        """Re-rank v√† ch·ªçn ra top_k t√†i li·ªáu li√™n quan nh·∫•t."""
        if not results:
            return [], []

        doc_ids = [int(str(r.get("title", "Document 0")).split()[-1]) for r in results]
        docs_text = "".join(f"[Document {doc_id}]\n{result.get('information', '')}\n\n" for doc_id, result in zip(doc_ids, results))

        rerank_prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia Re-ranker AI. Nhi·ªám v·ª• c·ªßa b·∫°n l√† s·∫Øp x·∫øp l·∫°i v√† ch·ªçn ra `{top_k}` t√†i li·ªáu li√™n quan nh·∫•t t·ª´ danh s√°ch cho tr∆∞·ªõc ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
C√ÇU H·ªéI: {query}
DANH S√ÅCH T√ÄI LI·ªÜU:
{docs_text}
---
Y√äU C·∫¶U:
1. Ph√¢n t√≠ch c√¢u h·ªèi v√† ƒë√°nh gi√° t·ª´ng t√†i li·ªáu.
2. S·∫Øp x·∫øp c√°c t√†i li·ªáu theo m·ª©c ƒë·ªô li√™n quan gi·∫£m d·∫ßn v√† ch·ªçn ra {top_k} t√†i li·ªáu t·ªët nh·∫•t.
3. Tr·∫£ v·ªÅ CH·ªà m·ªôt JSON object ch·ª©a danh s√°ch c√°c ID t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp l·∫°i.
FORMAT OUTPUT (TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T):
{{"selected_indices": [15, 3, 27, 8, 12, 1, 5]}}
"""
        messages = [
            {"role": "system", "content": f"B·∫°n l√† m·ªôt chuy√™n gia Re-ranker AI. Ch·ªâ tr·∫£ v·ªÅ JSON object v·ªõi format y√™u c·∫ßu."},
            {"role": "user", "content": rerank_prompt}
        ]

        try:
            response = self.nim.chat(messages, temperature=0.0, max_tokens=200)
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                parsed = json.loads(json_str)
                selected_doc_ids = parsed.get('selected_indices', [])
            else:
                raise ValueError("Invalid JSON response")

            reranked_results = []
            seen_doc_ids = set()
            original_results_map = {int(str(r.get("title", "Document 0")).split()[-1]): r for r in results}

            for doc_id in selected_doc_ids:
                if doc_id in original_results_map and doc_id not in seen_doc_ids:
                    reranked_results.append(original_results_map[doc_id])
                    seen_doc_ids.add(doc_id)
            
            # Fill remaining spots if LLM returns fewer than top_k
            if len(reranked_results) < top_k:
                for doc_id, result in original_results_map.items():
                    if doc_id not in seen_doc_ids:
                        reranked_results.append(result)
                        if len(reranked_results) == top_k:
                            break
            
            final_results = reranked_results[:top_k]
            final_doc_ids = [int(str(r.get("title", "Document 0")).split()[-1]) for r in final_results]
            return final_results, final_doc_ids
        except Exception as e:
            print(f"‚ùå Reranking failed: {e}. Falling back to original order.")
            return results[:top_k], doc_ids[:top_k]

# --- 5. QUERY CLASSIFIER ---
class QueryClassifier:
    """Ph√¢n lo·∫°i c√¢u h·ªèi ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ s·ª≠ d·ª•ng RAG hay kh√¥ng."""
    def __init__(self, nim_client: NIMClient):
        self.nim = nim_client
        self.proptit_keywords = [
            "proptit", "clb", "c√¢u l·∫°c b·ªô", "l·∫≠p tr√¨nh ptit", "ptit",
            "tuy·ªÉn th√†nh vi√™n", "ctv", "c·ªông t√°c vi√™n", "th√†nh vi√™n",
            "team", "ban", "d·ª± √°n", "ƒë√†o t·∫°o", "training", "ph·ªèng v·∫•n",
            "s·ª± ki·ªán", "event", "workshop", "cu·ªôc thi"
        ]

    def is_proptit_related(self, query: str) -> bool:
        """Ki·ªÉm tra nhanh b·∫±ng t·ª´ kh√≥a."""
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in self.proptit_keywords):
            return True
        
        # N·∫øu kh√¥ng c√≥ t·ª´ kh√≥a, d√πng LLM ƒë·ªÉ ch·∫Øc ch·∫Øn
        prompt = f"""B·∫°n l√† m·ªôt b·ªô ph√¢n lo·∫°i vƒÉn b·∫£n. Nhi·ªám v·ª• c·ªßa b·∫°n l√† x√°c ƒë·ªãnh xem c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng c√≥ li√™n quan ƒë·∫øn "C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh PTIT (ProPTIT)" hay kh√¥ng.
Ch·ªâ tr·∫£ l·ªùi "yes" ho·∫∑c "no".
C√¢u h·ªèi: "{query}"
C√¢u h·ªèi n√†y c√≥ li√™n quan ƒë·∫øn CLB L·∫≠p tr√¨nh PTIT kh√¥ng?
"""
        messages = [
            {"role": "system", "content": "B·∫°n l√† m·ªôt b·ªô ph√¢n lo·∫°i vƒÉn b·∫£n. Ch·ªâ tr·∫£ l·ªùi 'yes' ho·∫∑c 'no'."},
            {"role": "user", "content": prompt}
        ]
        try:
            response = self.nim.chat(messages, temperature=0.0, max_tokens=5).lower()
            return "yes" in response
        except Exception as e:
            print(f"‚ö†Ô∏è Query classification failed: {e}. Defaulting to RAG.")
            return True # M·∫∑c ƒë·ªãnh d√πng RAG n·∫øu c√≥ l·ªói

# --- 6. RAG PIPELINE ---
class RAGPipeline:
    """Orchestrates the entire RAG pipeline."""
    def __init__(self):
        self.nim_client = NIMClient()
        self.embedding_model = Embeddings()
        self.vector_db = VectorDatabaseAtlas()
        self.reranker = LLMReranker(self.nim_client)
        self.classifier = QueryClassifier(self.nim_client)
        self.rag_prompt_template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.
NGUY√äN T·∫ÆC TR·∫¢ L·ªúI B·∫ÆT BU·ªòC:
1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. KH√îNG ƒë∆∞·ª£c th√™m th√¥ng tin ngo√†i context.
2. Tr·∫£ l·ªùi CH√çNH X√ÅC, v√† TR·ª∞C TI·∫æP v√†o c√¢u h·ªèi.
3. KH√îNG ƒë∆∞·ª£c th√™m l·ªùi ch√†o h·ªèi, c·∫£m ∆°n, ho·∫∑c c√¢u x√£ giao kh√¥ng c·∫ßn thi·∫øt.
4. KH√îNG ƒë∆∞·ª£c n√≥i "Xin l·ªói", "T√¥i kh√¥ng bi·∫øt", "Kh√¥ng c√≥ th√¥ng tin" - PH·∫¢I tr·∫£ l·ªùi d·ª±a tr√™n context c√≥ s·∫µn.
5. N·∫øu context kh√¥ng ƒë·ªß, h√£y suy lu·∫≠n LOGIC t·ª´ th√¥ng tin c√≥ s·∫µn m√† KH√îNG b·ªãa th√™m.
6. T·∫≠p trung tr·∫£ l·ªùi C√ÇU H·ªéI CH√çNH, b·ªè qua th√¥ng tin kh√¥ng li√™n quan.
7. S·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, d·ªÖ hi·ªÉu, ph√π h·ª£p v·ªõi phong c√°ch tr·∫£ l·ªùi c·ªßa con ng∆∞·ªùi.
8. ∆Øu ti√™n x∆∞ng l√† "CLB" khi n√≥i v·ªÅ t·ªï ch·ª©c.
9. Kh√¥ng ƒë∆∞·ª£c th√™m c√¢u d·∫´n nh∆∞ "D·ª±a tr√™n th√¥ng tin t·ª´ ng·ªØ c·∫£nh, d∆∞·ªõi ƒë√¢y l√†...", tr·∫£ l·ªùi tr·ª±c ti·∫øp v√†o c√¢u h·ªèi
{context}
D·ª±a v√†o th√¥ng tin tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau:
C√¢u h·ªèi: {query}
"""

    def get_response(self, query: str, chat_history: List[Dict[str, str]]) -> Tuple[str, Dict]:
        """
        Main function to get a response for a user query.
        Returns the response string and a log dictionary.
        """
        logs = {"query": query, "rag_enabled": False}

        # Ki·ªÉm tra n·∫øu ƒëang ·ªü demo mode
        if hasattr(self.vector_db, 'is_demo_mode') and self.vector_db.is_demo_mode:
            logs["classification"] = "Demo Mode"
            response = "Xin ch√†o! Chatbot ƒëang ch·∫°y ·ªü ch·∫ø ƒë·ªô demo do thi·∫øu c·∫•u h√¨nh database. ƒê·ªÉ s·ª≠ d·ª•ng ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng RAG, vui l√≤ng c·∫•u h√¨nh MONGODB_URI v√† NIM_API_KEY trong secrets c·ªßa Hugging Face Space."
            logs["response"] = response
            return response, logs

        if not self.classifier.is_proptit_related(query):
            logs["classification"] = "General Conversation"
            response = self.get_general_response(query, chat_history)
            logs["response"] = response
            return response, logs

        logs["rag_enabled"] = True
        logs["classification"] = "PROPTIT-related"

        # 1. Vector Search
        k_retrieval = 30
        query_embedding = self.embedding_model.encode(query).tolist()
        
        vector_results = self.vector_db.vector_search(query_embedding, limit=k_retrieval)
        
        retrieved_ids = [int(str(r.get("title", "Doc 0")).split()[-1]) for r in vector_results]
        logs["vector_search"] = {
            "retrieved_ids": retrieved_ids,
            "retrieved_count": len(vector_results)
        }

        # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm, tr·∫£ l·ªùi m·∫∑c ƒë·ªãnh
        if not vector_results:
            logs["response"] = "Xin l·ªói, hi·ªán t·∫°i t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i sau."
            return logs["response"], logs

        # 2. Rerank
        k_final = 7
        reranked_results, reranked_ids = self.reranker.rerank(query, vector_results, top_k=k_final)
        logs["rerank"] = {"reranked_ids": reranked_ids}

        # 3. Generate Response
        context = "\n\n".join([f"Tr√≠ch ƒëo·∫°n t·ª´ t√†i li·ªáu {reranked_ids[i]}:\n{doc.get('information', '')}" for i, doc in enumerate(reranked_results)])
        
        final_prompt = self.rag_prompt_template.format(context=context, query=query)
        
        messages = self._build_chat_history(final_prompt, chat_history)
        
        response = self.nim_client.chat(messages)
        logs["response"] = response
        logs["final_context"] = context

        return response, logs

    def get_general_response(self, query: str, chat_history: List[Dict[str, str]]) -> str:
        """Generate a response for non-RAG questions."""
        prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán c·ªßa CLB L·∫≠p tr√¨nh PTIT.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n.
C√¢u h·ªèi: {query}
"""
        messages = self._build_chat_history(prompt, chat_history)
        return self.nim_client.chat(messages, temperature=0.5)

    def _build_chat_history(self, new_prompt: str, chat_history: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Builds the message list for the LLM, including history."""
        messages = [{"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI c·ªßa CLB L·∫≠p tr√¨nh PTIT."}]
        for message in chat_history[-4:]: # L·∫•y 4 c·∫∑p h·ªôi tho·∫°i g·∫ßn nh·∫•t
            messages.append({"role": "user", "content": message["user"]})
            messages.append({"role": "assistant", "content": message["assistant"]})
        messages.append({"role": "user", "content": new_prompt})
        return messages

# --- Main execution for testing ---
if __name__ == '__main__':
    print("Testing RAG Pipeline...")
    pipeline = RAGPipeline()
    
    test_query = "CLB c√≥ nh·ªØng ban n√†o?"
    print(f"\n--- Testing with query: '{test_query}' ---")
    response, logs = pipeline.get_response(test_query, [])
    print(f"Response: {response}")
    print(f"Logs: {json.dumps(logs, indent=2, ensure_ascii=False)}")

    test_query_general = "Tr·ªùi h√¥m nay ƒë·∫πp qu√°!"
    print(f"\n--- Testing with query: '{test_query_general}' ---")
    response, logs = pipeline.get_response(test_query_general, [])
    print(f"Response: {response}")
    print(f"Logs: {json.dumps(logs, indent=2, ensure_ascii=False)}")
