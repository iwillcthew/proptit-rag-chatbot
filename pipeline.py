# pipeline.py
import os
import json
import time
import hashlib
from typing import List, Dict, Any, Tuple, Union
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer

# Táº£i biáº¿n mÃ´i trÆ°á»ng tá»« file .env
load_dotenv()

# --- 1. NVIDIA NIM CLIENT ---
class NIMClient:
    """Client Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i NVIDIA NIM qua API tÆ°Æ¡ng thÃ­ch OpenAI."""
    def __init__(self, api_key: str = "", base_url: str = "", model: str = "meta/llama-3.1-405b-instruct"):
        api_key = api_key or os.environ.get("NIM_API_KEY", "")
        base_url = base_url or os.environ.get("NIM_BASE_URL", "https://integrate.api.nvidia.com/v1")
        if not api_key:
            print("âš ï¸ NIM_API_KEY khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y. Chatbot sáº½ cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo.")
            # Thay vÃ¬ raise error, chÃºng ta sáº½ sá»­ dá»¥ng má»™t API key demo hoáº·c fallback
            api_key = "demo_key"  # Placeholder - báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh logic nÃ y
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.is_demo_mode = api_key == "demo_key"

    def chat(self, messages, temperature: float = 0.1, max_tokens: int = 1024, top_p: float = 0.7, **kwargs) -> str:
        """Gá»­i yÃªu cáº§u chat vÃ  nháº­n pháº£n há»“i."""
        if self.is_demo_mode:
            return "Xin lá»—i, chatbot Ä‘ang á»Ÿ cháº¿ Ä‘á»™ demo do thiáº¿u cáº¥u hÃ¬nh API key. Vui lÃ²ng liÃªn há»‡ admin Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£."
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                    stream=False,
                    **kwargs
                )
                return (resp.choices[0].message.content or "").strip()
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"âŒ API call failed after {max_retries} attempts: {e}")
                    return f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n: {str(e)}"
                print(f"âš ï¸ API call failed (attempt {attempt + 1}/{max_retries}), retrying...")
                time.sleep(2 ** attempt)
        return "Xin lá»—i, khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n lÃºc nÃ y."

# --- 2. EMBEDDING MODEL ---
class Embeddings:
    """Táº£i vÃ  sá»­ dá»¥ng model embedding tá»« HuggingFace."""
    def __init__(self, model_path="iwillcthew/vietnamese-embedding-PROPTIT-domain-ft"):
        print(f"ğŸš€ Loading embedding model: {model_path}")
        self.model = SentenceTransformer(model_path)
        self.model.max_seq_length = 2048
        print("âœ… Embedding model loaded.")

    def encode(self, text: Union[str, List[str]]):
        """Táº¡o embedding cho vÄƒn báº£n."""
        return self.model.encode(text)

# --- 3. VECTOR DATABASE ---
class VectorDatabaseAtlas:
    """Káº¿t ná»‘i vÃ  truy váº¥n MongoDB Atlas Vector Search."""
    def __init__(self, mongodb_uri: str = ""):
        mongodb_uri = mongodb_uri or os.environ.get("MONGODB_URI", "")
        if not mongodb_uri:
            print("âš ï¸ MONGODB_URI khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y. Vector search sáº½ khÃ´ng kháº£ dá»¥ng.")
            self.client = None
            self.db = None
            self.collection = None
            self.is_demo_mode = True
            return
        
        try:
            print("ğŸ”— Connecting to MongoDB Atlas...")
            self.client = MongoClient(mongodb_uri)
            self.db = self.client.get_database("vector_db")
            self.collection = self.db["information"]
            self.is_demo_mode = False
            print("âœ… MongoDB Atlas connected.")
        except Exception as e:
            print(f"âš ï¸ Failed to connect to MongoDB: {e}")
            self.client = None
            self.db = None
            self.collection = None
            self.is_demo_mode = True

    def vector_search(self, query_vector: List[float], limit: int = 30) -> List[Dict]:
        """Thá»±c hiá»‡n vector search."""
        if self.is_demo_mode or self.collection is None:
            print("âš ï¸ Vector search khÃ´ng kháº£ dá»¥ng trong cháº¿ Ä‘á»™ demo.")
            return []
        
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "vector_index",
                    "queryVector": query_vector,
                    "path": "embedding",
                    "numCandidates": max(200, limit * 10),
                    "limit": limit
                }
            }
        ]
        try:
            return list(self.collection.aggregate(pipeline, maxTimeMS=30000))
        except Exception as e:
            print(f"âš ï¸ Vector search failed: {e}")
            return []

# --- 4. LLM RERANKER ---
class LLMReranker:
    """Sá»­ dá»¥ng LLM Ä‘á»ƒ re-rank káº¿t quáº£ tÃ¬m kiáº¿m."""
    def __init__(self, nim_client: NIMClient):
        self.nim = nim_client

    def rerank(self, query: str, results: List[Dict[str, Any]], top_k: int = 7, chat_history: List[Dict[str, str]] = None) -> Tuple[List[Dict[str, Any]], List[int]]:
        """Re-rank vÃ  chá»n ra top_k tÃ i liá»‡u liÃªn quan nháº¥t, cÃ³ xÃ©t Ä‘áº¿n lá»‹ch sá»­ há»™i thoáº¡i."""
        if not results:
            return [], []

        doc_ids = [int(str(r.get("title", "Document 0")).split()[-1]) for r in results]
        docs_text = "".join(f"[Document {doc_id}]\n{result.get('information', '')}\n\n" for doc_id, result in zip(doc_ids, results))

        # XÃ¢y dá»±ng ngá»¯ cáº£nh tá»« lá»‹ch sá»­ chat
        context_str = ""
        if chat_history:
            context_str = "NGá»® Cáº¢NH Há»˜I THOáº I:\n"
            for i, turn in enumerate(chat_history[-2:]):  # Láº¥y 2 lÆ°á»£t gáº§n nháº¥t
                context_str += f"NgÆ°á»i dÃ¹ng: {turn['user']}\nTrá»£ lÃ½: {turn['assistant']}\n"
            context_str += "---\n"
            print(f"ğŸ”„ [RERANKER] Chat context Ä‘Æ°á»£c táº¡o:")
            print(f"{context_str}")
        else:
            print(f"ğŸ”„ [RERANKER] KhÃ´ng cÃ³ chat history")

        rerank_prompt = f"""Báº¡n lÃ  má»™t chuyÃªn gia Re-ranker AI. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  sáº¯p xáº¿p láº¡i vÃ  chá»n ra `{top_k}` tÃ i liá»‡u liÃªn quan nháº¥t tá»« danh sÃ¡ch cho trÆ°á»›c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.

{context_str}CÃ‚U Há»I HIá»†N Táº I: {query}

DANH SÃCH TÃ€I LIá»†U:
{docs_text}
---
YÃŠU Cáº¦U:
1. PhÃ¢n tÃ­ch cÃ¢u há»i hiá»‡n táº¡i dá»±a trÃªn ngá»¯ cáº£nh há»™i thoáº¡i (náº¿u cÃ³) vÃ  Ä‘Ã¡nh giÃ¡ tá»«ng tÃ i liá»‡u.
2. Sáº¯p xáº¿p cÃ¡c tÃ i liá»‡u theo má»©c Ä‘á»™ liÃªn quan giáº£m dáº§n vÃ  chá»n ra {top_k} tÃ i liá»‡u tá»‘t nháº¥t, liÃªn quan nháº¥t, cÃ³ thá»ƒ giÃºp tráº£ lá»i cÃ¢u há»i.
3. Tráº£ vá» CHá»ˆ má»™t JSON object chá»©a danh sÃ¡ch cÃ¡c ID tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c sáº¯p xáº¿p láº¡i.
FORMAT OUTPUT (TUÃ‚N THá»¦ NGHIÃŠM NGáº¶T):
{{"selected_indices": [15, 3, 27, 8, 12, 1, 5]}}
"""
        messages = [
            {"role": "system", "content": f"Báº¡n lÃ  má»™t chuyÃªn gia Re-ranker AI. Chá»‰ tráº£ vá» JSON object vá»›i format yÃªu cáº§u."},
            {"role": "user", "content": rerank_prompt}
        ]

        print(f"\n{'='*60}")
        print(f"ğŸ”„ [RERANKER] PROMPT Gá»¬I Äáº¾N LLM:")
        print(f"{'='*60}")
        print(f"System: {messages[0]['content']}")
        print(f"\nUser: {messages[1]['content']}")
        print(f"{'='*60}\n")

        try:
            response = self.nim.chat(messages, temperature=0.0, max_tokens=200)
            print(f"ğŸ”„ [RERANKER] RESPONSE Tá»ª LLM:")
            print(f"Raw response: {response}")
            print(f"{'='*60}\n")
            
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                parsed = json.loads(json_str)
                selected_doc_ids = parsed.get('selected_indices', [])
            else:
                raise ValueError("Invalid JSON response")

            reranked_results = []
            seen_doc_ids = set()
            original_results_map = {int(str(r.get("title", "Document 0")).split()[-1]): r for r in results}

            for doc_id in selected_doc_ids:
                if doc_id in original_results_map and doc_id not in seen_doc_ids:
                    reranked_results.append(original_results_map[doc_id])
                    seen_doc_ids.add(doc_id)
            
            # Fill remaining spots if LLM returns fewer than top_k
            if len(reranked_results) < top_k:
                for doc_id, result in original_results_map.items():
                    if doc_id not in seen_doc_ids:
                        reranked_results.append(result)
                        if len(reranked_results) == top_k:
                            break
            
            final_results = reranked_results[:top_k]
            final_doc_ids = [int(str(r.get("title", "Document 0")).split()[-1]) for r in final_results]
            return final_results, final_doc_ids
        except Exception as e:
            print(f"âŒ Reranking failed: {e}. Falling back to original order.")
            return results[:top_k], doc_ids[:top_k]

# --- 5. QUERY CLASSIFIER ---
class QueryClassifier:
    """PhÃ¢n loáº¡i cÃ¢u há»i Ä‘á»ƒ quyáº¿t Ä‘á»‹nh cÃ³ sá»­ dá»¥ng RAG hay khÃ´ng, cÃ³ xÃ©t Ä‘áº¿n lá»‹ch sá»­ há»™i thoáº¡i."""
    def __init__(self, nim_client: NIMClient):
        self.nim = nim_client
        self.proptit_keywords = [
            "proptit", "clb", "cÃ¢u láº¡c bá»™", "láº­p trÃ¬nh ptit", "ptit",
            "tuyá»ƒn thÃ nh viÃªn", "ctv", "cá»™ng tÃ¡c viÃªn", "thÃ nh viÃªn",
            "team", "ban", "dá»± Ã¡n", "Ä‘Ã o táº¡o", "training", "phá»ng váº¥n",
            "sá»± kiá»‡n", "event", "workshop", "cuá»™c thi", "PROGAP"
        ]

    def is_proptit_related(self, query: str, chat_history: List[Dict[str, str]] = None) -> bool:
        """
        Kiá»ƒm tra xem cÃ¢u há»i cÃ³ liÃªn quan Ä‘áº¿n PROPTIT khÃ´ng, sá»­ dá»¥ng tá»« khÃ³a vÃ  LLM vá»›i ngá»¯ cáº£nh.
        """
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in self.proptit_keywords):
            print(f"ğŸ” [QueryClassifier] Keyword match found for: '{query}'")
            return True
        
        # Debug: In ra lá»‹ch sá»­ chat
        print(f"ğŸ” [QueryClassifier] No keyword match for: '{query}', checking with LLM...")
        print(f"ğŸ” [QueryClassifier] Chat history length: {len(chat_history) if chat_history else 0}")
        
        # Náº¿u khÃ´ng cÃ³ tá»« khÃ³a, dÃ¹ng LLM Ä‘á»ƒ cháº¯c cháº¯n, cÃ³ kÃ¨m lá»‹ch sá»­ chat
        history_str = ""
        if chat_history:
            for i, turn in enumerate(chat_history[-3:]):  # Láº¥y 3 lÆ°á»£t há»™i thoáº¡i gáº§n nháº¥t
                history_str += f"NgÆ°á»i dÃ¹ng: {turn['user']}\nTrá»£ lÃ½: {turn['assistant']}\n"
                print(f"ğŸ” [QueryClassifier] History {i+1}: User='{turn['user'][:50]}...', Assistant='{turn['assistant'][:50]}...'")

        prompt = f"""Báº¡n lÃ  má»™t bá»™ phÃ¢n loáº¡i vÄƒn báº£n. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  xÃ¡c Ä‘á»‹nh xem CÃ‚U Há»I CUá»I CÃ™NG cá»§a ngÆ°á»i dÃ¹ng cÃ³ liÃªn quan Ä‘áº¿n "CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh PTIT (ProPTIT)" hay khÃ´ng, dá»±a vÃ o lá»‹ch sá»­ há»™i thoáº¡i náº¿u cÃ³.
Chá»‰ tráº£ lá»i "yes" hoáº·c "no".

---
Lá»ŠCH Sá»¬ Há»˜I THOáº I (náº¿u cÃ³):
{history_str if history_str else "KhÃ´ng cÃ³"}
---
CÃ‚U Há»I CUá»I CÃ™NG: "{query}"
---

Dá»±a vÃ o cáº£ lá»‹ch sá»­ há»™i thoáº¡i, cÃ¢u há»i cuá»‘i cÃ¹ng cÃ³ liÃªn quan Ä‘áº¿n CLB Láº­p trÃ¬nh PTIT khÃ´ng?
"""
        messages = [
            {"role": "system", "content": "Báº¡n lÃ  má»™t bá»™ phÃ¢n loáº¡i vÄƒn báº£n. Chá»‰ tráº£ lá»i 'yes' hoáº·c 'no'."},
            {"role": "user", "content": prompt}
        ]
        try:
            response = self.nim.chat(messages, temperature=0.0, max_tokens=5).lower()
            print(f"ğŸ” [QueryClassifier] LLM response: '{response}'")
            result = "yes" in response
            print(f"ğŸ” [QueryClassifier] Final decision: {result}")
            return result
        except Exception as e:
            print(f"âš ï¸ Query classification with context failed: {e}. Defaulting to RAG.")
            return True # Máº·c Ä‘á»‹nh dÃ¹ng RAG náº¿u cÃ³ lá»—i

# --- 6. RAG PIPELINE ---
class RAGPipeline:
    """Orchestrates the entire RAG pipeline."""
    def __init__(self):
        self.nim_client = NIMClient()
        self.embedding_model = Embeddings()
        self.vector_db = VectorDatabaseAtlas()
        self.reranker = LLMReranker(self.nim_client)
        self.classifier = QueryClassifier(self.nim_client)
        self.rag_prompt_template = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
5. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
6. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
7. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
8. KhÃ´ng Ä‘Æ°á»£c thÃªm cÃ¢u dáº«n nhÆ° "Dá»±a trÃªn thÃ´ng tin tá»« ngá»¯ cáº£nh, dÆ°á»›i Ä‘Ã¢y lÃ ...", tráº£ lá»i trá»±c tiáº¿p vÃ o cÃ¢u há»i.
9. QUAN TRá»ŒNG: Xem xÃ©t lá»‹ch sá»­ há»™i thoáº¡i Ä‘á»ƒ hiá»ƒu Ä‘Ãºng ngá»¯ cáº£nh cá»§a cÃ¢u há»i hiá»‡n táº¡i.

---
Context:
{context}
---
Lá»‹ch sá»­ chat:
{chat_context}
---
Dá»±a vÃ o thÃ´ng tin trÃªn, hÃ£y tráº£ lá»i chi tiáº¿t, Ä‘áº§y Ä‘á»§ cÃ¢u há»i sau:
CÃ¢u há»i hiá»‡n táº¡i: {query}
"""

    def get_response(self, query: str, chat_history: List[Dict[str, str]]) -> Tuple[str, Dict]:
        """
        Main function to get a response for a user query.
        Returns the response string and a log dictionary.
        """
        logs = {"query": query, "rag_enabled": False}

        # Kiá»ƒm tra náº¿u Ä‘ang á»Ÿ demo mode
        if hasattr(self.vector_db, 'is_demo_mode') and self.vector_db.is_demo_mode:
            logs["classification"] = "Demo Mode"
            response = "Xin chÃ o! Chatbot Ä‘ang cháº¡y á»Ÿ cháº¿ Ä‘á»™ demo do thiáº¿u cáº¥u hÃ¬nh database. Äá»ƒ sá»­ dá»¥ng Ä‘áº§y Ä‘á»§ tÃ­nh nÄƒng RAG, vui lÃ²ng cáº¥u hÃ¬nh MONGODB_URI vÃ  NIM_API_KEY trong secrets cá»§a Hugging Face Space."
            logs["response"] = response
            return response, logs

        if not self.classifier.is_proptit_related(query, chat_history):
            logs["classification"] = "General Conversation"
            response = self.get_general_response(query, chat_history)
            logs["response"] = response
            return response, logs

        logs["rag_enabled"] = True
        logs["classification"] = "PROPTIT-related"

        # 1. Vector Search
        k_retrieval = 30
        query_embedding = self.embedding_model.encode(query).tolist()
        
        vector_results = self.vector_db.vector_search(query_embedding, limit=k_retrieval)
        
        retrieved_ids = [int(str(r.get("title", "Doc 0")).split()[-1]) for r in vector_results]
        logs["vector_search"] = {
            "retrieved_ids": retrieved_ids,
            "retrieved_count": len(vector_results)
        }

        # Náº¿u khÃ´ng cÃ³ káº¿t quáº£ tÃ¬m kiáº¿m, tráº£ lá»i máº·c Ä‘á»‹nh
        if not vector_results:
            logs["response"] = "Xin lá»—i, hiá»‡n táº¡i tÃ´i khÃ´ng thá»ƒ tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n. Vui lÃ²ng thá»­ láº¡i sau."
            return logs["response"], logs

        # 2. Rerank
        k_final = 7
        reranked_results, reranked_ids = self.reranker.rerank(query, vector_results, top_k=k_final, chat_history=chat_history)
        logs["rerank"] = {"reranked_ids": reranked_ids}

        # 3. Generate Response
        context = "\n\n".join([f"TrÃ­ch Ä‘oáº¡n tá»« tÃ i liá»‡u {reranked_ids[i]}:\n{doc.get('information', '')}" for i, doc in enumerate(reranked_results)])
        
        # XÃ¢y dá»±ng ngá»¯ cáº£nh chat cho generation
        chat_context_str = ""
        if chat_history:
            chat_context_str = "Lá»ŠCH Sá»¬ Há»˜I THOáº I:\n"
            for turn in chat_history[-2:]:  # Láº¥y 2 lÆ°á»£t gáº§n nháº¥t
                chat_context_str += f"NgÆ°á»i dÃ¹ng: {turn['user']}\nTrá»£ lÃ½: {turn['assistant']}\n"
            chat_context_str += "\n"
            print(f"ğŸ“ [GENERATION] Chat context Ä‘Æ°á»£c táº¡o:")
            print(f"{chat_context_str}")
        else:
            print(f"ğŸ“ [GENERATION] KhÃ´ng cÃ³ chat history")
        
        final_prompt = self.rag_prompt_template.format(
            chat_context=chat_context_str, 
            context=context, 
            query=query
        )
        
        messages = self._build_chat_history(final_prompt, chat_history)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ [GENERATION] PROMPT Gá»¬I Äáº¾N LLM:")
        print(f"{'='*60}")
        print(f"System: {messages[0]['content']}")
        print(f"\nCÃ¡c messages tá»« lá»‹ch sá»­:")
        for i, msg in enumerate(messages[1:-1]):
            print(f"Message {i+1} ({msg['role']}): {msg['content'][:100]}...")
        print(f"\nUser final: {messages[-1]['content']}")
        print(f"{'='*60}\n")
        
        response = self.nim_client.chat(messages)
        
        print(f"ğŸ“ [GENERATION] RESPONSE Tá»ª LLM:")
        print(f"Final response: {response}")
        print(f"{'='*60}\n")
        logs["response"] = response
        logs["final_context"] = context

        return response, logs

    def get_general_response(self, query: str, chat_history: List[Dict[str, str]]) -> str:
        """Generate a response for non-RAG questions."""
        prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ¢n thiá»‡n cá»§a CLB Láº­p trÃ¬nh PTIT.
HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch tá»± nhiÃªn.
CÃ¢u há»i: {query}
"""
        messages = self._build_chat_history(prompt, chat_history)
        return self.nim_client.chat(messages, temperature=0.5)

    def _build_chat_history(self, new_prompt: str, chat_history: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Builds the message list for the LLM, including history."""
        messages = [{"role": "system", "content": "Báº¡n lÃ  má»™t trá»£ lÃ½ AI cá»§a CLB Láº­p trÃ¬nh PTIT."}]
        for message in chat_history[-4:]: # Láº¥y 4 cáº·p há»™i thoáº¡i gáº§n nháº¥t
            messages.append({"role": "user", "content": message["user"]})
            messages.append({"role": "assistant", "content": message["assistant"]})
        messages.append({"role": "user", "content": new_prompt})
        return messages

# --- Main execution for testing ---
if __name__ == '__main__':
    print("Testing RAG Pipeline...")
    pipeline = RAGPipeline()
    
    test_query = "CLB cÃ³ nhá»¯ng ban nÃ o?"
    print(f"\n--- Testing with query: '{test_query}' ---")
    response, logs = pipeline.get_response(test_query, [])
    print(f"Response: {response}")
    print(f"Logs: {json.dumps(logs, indent=2, ensure_ascii=False)}")

    test_query_general = "Trá»i hÃ´m nay Ä‘áº¹p quÃ¡!"
    print(f"\n--- Testing with query: '{test_query_general}' ---")
    response, logs = pipeline.get_response(test_query_general, [])
    print(f"Response: {response}")
    print(f"Logs: {json.dumps(logs, indent=2, ensure_ascii=False)}")