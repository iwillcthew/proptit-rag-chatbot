#metric.py
import os
import time
import json
import math
import hashlib
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple
from openai import OpenAI
from dotenv import load_dotenv
import re
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

load_dotenv()

class NIMClient:
    """NVIDIA NIM qua OpenAI-compatible API."""
    def __init__(self,
                 api_key: str = "",
                 base_url: str = "",
                 model: str = "meta/llama-3.1-405b-instruct"):
        api_key = api_key or os.environ.get("NIM_API_KEY", "")
        base_url = base_url or os.environ.get("NIM_BASE_URL", "https://integrate.api.nvidia.com/v1")
        if not api_key:
            raise RuntimeError("âŒ NIM_API_KEY chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p. Set env var NIM_API_KEY hoáº·c truyá»n trá»±c tiáº¿p.")
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model

    def chat(self,
             messages,
             temperature: float = 0.0,
             max_tokens: int = 512,
             top_p: float = 0.7,
             **kwargs) -> str:
        """Gá»i API vá»›i retry logic."""
        max_retries = 5
        for attempt in range(max_retries):
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                    stream=False,
                    **kwargs
                )
                return (resp.choices[0].message.content or "").strip()
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"âŒ API call failed after {max_retries} attempts: {e}")
                    raise e
                print(f"âš ï¸ API call failed (attempt {attempt + 1}/{max_retries}), retrying...")
                time.sleep(2 ** attempt)

class Embeddings:
    def __init__(self, model_path="iwillcthew/vietnamese-embedding-PROPTIT-domain-ft"):
        """Load embedding model from HuggingFace."""
        self.model_path = model_path
        self.model = None

        print(f"ğŸš€ Loading embedding model: {model_path}")
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_path)

        self.model.max_seq_length = 2048
        print("âœ… Embedding model loaded.")

    def encode(self, text):
        """Encode text(s) for documents."""
        if isinstance(text, str):
            return self.model.encode([text])[0]
        return self.model.encode(text)

    def encode_query(self, query):
        """Encode query for search."""
        return self.model.encode([query])[0]

from pymongo import MongoClient

class VectorDatabaseAtlas:
    def __init__(self, mongodb_uri: str = ""):
        mongodb_uri = mongodb_uri or os.environ.get("MONGODB_URI", "")
        if not mongodb_uri:
            raise RuntimeError("âŒ MONGODB_URI not found. Please set it in .env file or pass directly.")
        print("ğŸ”— Connecting to MongoDB Atlas...")
        self.client = MongoClient(mongodb_uri)
        self.db = self.client.get_database("vector_db")
        print("âœ… MongoDB Atlas connected.")

    def insert_document(self, collection_name: str, document: dict):
        self.db[collection_name].insert_one(document)

    def query(self, collection_name: str, query_vector: List[float], limit: int = 5):
        num_candidates = max(200, limit * 10)
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "vector_index",
                    "queryVector": query_vector,
                    "path": "embedding",
                    "numCandidates": num_candidates,
                    "limit": limit
                }
            }
        ]

        attempts = 3
        backoff = 1
        max_time_ms = 30000  

        for attempt in range(1, attempts + 1):
            try:
                cursor = self.db[collection_name].aggregate(pipeline, maxTimeMS=max_time_ms)
                results = list(cursor)
                return results
            except Exception as e:
                print(f"âš ï¸ Vector query attempt {attempt}/{attempts} failed: {e}")
                if attempt == attempts:
                    print("âŒ Vector query failed after retries â€” returning empty list as fallback.")
                    return []
                num_candidates = max(50, num_candidates // 2)
                pipeline[0]["$vectorSearch"]["numCandidates"] = num_candidates
                max_time_ms = max(5000, int(max_time_ms * 0.5))
                time.sleep(backoff)
                backoff *= 2

    def count_documents(self, collection_name: str) -> int:
        return self.db[collection_name].count_documents({})

    def drop_collection(self, collection_name: str):
        self.db[collection_name].drop()

class LLMReranker:
    def __init__(self, nim_client: NIMClient):
        self.nim = nim_client
        print("ğŸš€ Initialized LLM-based reranker")

    def rerank_results(self, query: str, results: List[Dict[str, Any]], top_k: int = None):
        """Use LLM to rerank and select the most relevant documents."""
        if not results or len(results) <= 1:
            return results

        print(f"ğŸ”„ Reranking {len(results)} results...")

        doc_ids = []
        for result in results:
            if "title" in result:
                try:
                    doc_id = int(str(result["title"]).split()[-1])
                    doc_ids.append(doc_id)
                except:
                    doc_ids.append(0)
            else:
                doc_ids.append(0)

        docs_text = ""
        for i, (result, doc_id) in enumerate(zip(results, doc_ids)):
            doc_content = result.get("information", "")
            docs_text += f"[Document {doc_id}]\n{doc_content}\n\n"
        rerank_prompt = f"""Báº¡n lÃ  má»™t chuyÃªn gia Re-ranker AI hÃ ng Ä‘áº§u. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  sáº¯p xáº¿p láº¡i vÃ  chá»n ra `{top_k}` tÃ i liá»‡u liÃªn quan nháº¥t tá»« má»™t danh sÃ¡ch cho trÆ°á»›c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.

BÆ¯á»šC RERANK: PhÃ¢n tÃ­ch vÃ  chá»n ra {top_k} tÃ i liá»‡u PHÃ™ Há»¢P NHáº¤T Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.

CÃ‚U Há»I: {query}

DANH SÃCH TÃ€I LIá»†U:
{docs_text}
---
Báº¡n pháº£i thá»±c hiá»‡n theo quy trÃ¬nh sau:
- **PhÃ¢n tÃ­ch cÃ¢u há»i:** Hiá»ƒu rÃµ Ã½ Ä‘á»‹nh, cÃ¡c thá»±c thá»ƒ chÃ­nh vÃ  thÃ´ng tin mÃ  ngÆ°á»i dÃ¹ng Ä‘ang tÃ¬m kiáº¿m.
- **ÄÃ¡nh giÃ¡ tá»«ng tÃ i liá»‡u:** Äá»‘i chiáº¿u ná»™i dung má»—i tÃ i liá»‡u vá»›i cÃ¢u há»i dá»±a trÃªn cÃ¡c tiÃªu chÃ­ bÃªn dÆ°á»›i.
- **Sáº¯p xáº¿p vÃ  lá»±a chá»n:** Sáº¯p xáº¿p cÃ¡c tÃ i liá»‡u theo thá»© tá»± Æ°u tiÃªn giáº£m dáº§n vÃ  chá»n ra {top_k} tÃ i liá»‡u tá»‘t nháº¥t.

QUY Táº®C ÄÃNH GIÃ (Relevance Rubric 0â€“4, sá»­ dá»¥ng Ä‘á»ƒ xáº¿p háº¡ng â€“ KHÃ”NG in ra Ä‘iá»ƒm):
- 4 = Tráº£ lá»i trá»±c tiáº¿p/bao trÃ¹m cÃ¢u há»i; thÃ´ng tin chÃ­nh xÃ¡c, cá»¥ thá»ƒ, cÃ³ thá»ƒ dÃ¹ng Ä‘á»™c láº­p Ä‘á»ƒ tráº£ lá»i.
- 3 = LiÃªn quan máº¡nh, chá»©a pháº§n lá»›n thÃ´ng tin cáº§n thiáº¿t hoáº·c máº£nh ghÃ©p quan trá»ng Ä‘á»ƒ hoÃ n thiá»‡n cÃ¢u tráº£ lá»i.
- 2 = LiÃªn quan vá»«a; cÃ³ thÃ´ng tin ná»n/giÃ¡n tiáº¿p há»¯u Ã­ch (Ä‘á»‹nh nghÄ©a, bá»‘i cáº£nh, vÃ­ dá»¥) Ä‘á»ƒ há»— trá»£ tráº£ lá»i.
- 1 = LiÃªn quan yáº¿u; Ä‘á» cáº­p khÃ¡i quÃ¡t Ä‘áº¿n chá»§ Ä‘á» hoáº·c cÃ³ tá»« khÃ³a liÃªn quan Ä‘áº¿n cÃ¢u há»i nhÆ°ng thiáº¿u chi tiáº¿t há»¯u Ã­ch.
- 0 = KhÃ´ng liÃªn quan/ngoÃ i pháº¡m vi.

HÆ¯á»šNG DáºªN Lá»°A CHá»ŒN
1) Æ¯u tiÃªn tÃ i liá»‡u Ä‘iá»ƒm 4, sau Ä‘Ã³ 3, rá»“i 2. TrÃ¡nh chá»n 0â€“1 trá»« khi khÃ´ng cÃ³ lá»±a chá»n tá»‘t hÆ¡n.
2) Vá»›i cÃ¢u há»i Ä‘a khÃ­a cáº¡nh, cÃ³ thá»ƒ chá»n nhiá»u tÃ i liá»‡u bá»• trá»£, miá»…n tá»•ng thá»ƒ giÃºp tráº£ lá»i Ä‘áº§y Ä‘á»§.
3) KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thá»© tá»± xuáº¥t hiá»‡n cá»§a tÃ i liá»‡u.
4) Sáº¯p xáº¿p tÃ i liá»‡u theo má»©c Ä‘á»™ liÃªn quan giáº£m dáº§n (tÃ i liá»‡u liÃªn quan hÆ¡n Ä‘á»©ng trÆ°á»›c) (láº¥y chÃ­nh xÃ¡c {top_k} tÃ i liá»‡u liÃªn quan nháº¥t)
---
YÃŠU Cáº¦U FORMAT OUTPUT (TUÃ‚N THá»¦ NGHIÃŠM NGáº¶T):
Tráº£ vá» CHá»ˆ JSON object vá»›i format:
{{"selected_indices": [5,12,18,27,33]}}

Trong Ä‘Ã³:
- selected_indices: Máº£ng cÃ¡c ID tÃ i liá»‡u THá»°C Táº¾ (khÃ´ng pháº£i chá»‰ sá»‘ máº£ng)
- Chá»‰ tráº£ vá» JSON, khÃ´ng cÃ³ text khÃ¡c
- KhÃ´ng giáº£i thÃ­ch, khÃ´ng thÃªm kÃ½ tá»± nÃ o khÃ¡c

VÃ­ dá»¥ output Ä‘Ãºng:
{{"selected_indices": [15,3,27,8,12]}}"""

        messages = [
            {
                "role": "system",
                "content": f"Báº¡n lÃ  má»™t chuyÃªn gia Re-ranker AI hÃ ng Ä‘áº§u. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  sáº¯p xáº¿p láº¡i vÃ  chá»n ra `{top_k}` tÃ i liá»‡u liÃªn quan nháº¥t tá»« má»™t danh sÃ¡ch tÃ i liá»‡u cho trÆ°á»›c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng. LuÃ´n tráº£ vá» JSON format chÃ­nh xÃ¡c nhÆ° yÃªu cáº§u. KhÃ´ng thÃªm text nÃ o khÃ¡c."
            },
            {
                "role": "user",
                "content": rerank_prompt
            }
        ]

        try:
            response = self.nim.chat(messages, temperature=0.0, max_tokens=1000)

            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                parsed = json.loads(json_str)
                selected_doc_ids = parsed.get('selected_indices', [])
            else:
                parsed = json.loads(response.strip())
                selected_doc_ids = parsed.get('selected_indices', [])

            reranked_results = []
            used_doc_ids = set()

            for selected_id in selected_doc_ids:
                if isinstance(selected_id, int):
                    for i, (result, doc_id) in enumerate(zip(results, doc_ids)):
                        if doc_id == selected_id and selected_id not in used_doc_ids:
                            reranked_results.append(result)
                            used_doc_ids.add(selected_id)
                            break

            if len(reranked_results) < (top_k or len(results)):
                for i, (result, doc_id) in enumerate(zip(results, doc_ids)):
                    if doc_id not in used_doc_ids:
                        reranked_results.append(result)
                        used_doc_ids.add(doc_id)
                        if len(reranked_results) >= (top_k or len(results)):
                            break

            if top_k:
                reranked_results = reranked_results[:top_k]

            print(f"âœ… Reranking completed. Selected {len(reranked_results)} documents")
            return reranked_results

        except Exception as e:
            print(f"âŒ Reranking failed, using original order")
            return results[:top_k] if top_k else results

# Search cache vÃ  cÃ¡c utility functions
_search_cache: Dict[str, Dict[str, Any]] = {}

def clear_search_cache():
    global _search_cache
    _search_cache = {}
    print("ğŸ§¹ Cleared search cache.")

def extract_doc_ids(results: List[Dict[str, Any]]) -> List[int]:
    out = []
    for r in results:
        if "title" in r:
            try:
                out.append(int(str(r["title"]).split()[-1]))
            except:
                pass
    return out

def vector_search_only(query: str, embedding: Embeddings, vector_db, k: int = 5, nim_client: NIMClient = None):
    """Pure vector search without keyword search."""
    user_emb = embedding.encode_query(query)
    initial_k = 35
    raw = vector_db.query("information", user_emb.tolist(), limit=initial_k)

    if nim_client and len(raw) > k:
        reranked = LLMReranker(nim_client).rerank_results(query, raw, top_k=k)
    else:
        reranked = raw[:k]

    return user_emb, reranked

def get_cached_search_results(query: str, embedding: Embeddings, vector_db, k: int = 7, nim_client: NIMClient = None):
    global _search_cache
    key = hashlib.md5((query + f"_k{k}").encode("utf-8")).hexdigest()
    if key in _search_cache:
        return _search_cache[key]
    user_embedding, results = vector_search_only(query, embedding, vector_db, k, nim_client)
    retrieved_docs = extract_doc_ids(results)
    data = {"user_embedding": user_embedding, "results": results, "retrieved_docs": retrieved_docs}
    _search_cache[key] = data
    return data

# Metric primitives
def dcg_at_k(relevances, k):
    relevances = np.array(relevances)[:k]
    return float(np.sum((2**relevances - 1) / np.log2(np.arange(2, len(relevances) + 2))))

def ndcg_at_k(relevances, k):
    best = dcg_at_k(sorted(relevances, reverse=True), k)
    if best == 0:
        return 0.0
    return dcg_at_k(relevances, k) / best

def cosine01(a, b):
    a = np.array(a); b = np.array(b)
    na = np.linalg.norm(a); nb = np.linalg.norm(b)
    if na == 0 or nb == 0:
        return 0.0
    return float((np.dot(a, b) / (na * nb) + 1.0) / 2.0)

def _parse_ground_truth_docs(ground_truth_field) -> List[int]:
    docs = []
    if isinstance(ground_truth_field, str):
        for x in ground_truth_field.split(","):
            try:
                docs.append(int(x))
            except:
                pass
    else:
        try:
            docs.append(int(ground_truth_field))
        except:
            pass
    return docs

def hit_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    hits = 0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]

        hit_docs = [d for d in retrieved_docs if d in gt_docs]
        is_hit = len(hit_docs) > 0
        hits += int(is_hit)

    hit_rate = hits / n if n > 0 else 0.0
    print(f"ğŸ¯ Hit@{k}: {hits}/{n} = {hit_rate:.4f}")
    return hit_rate

def recall_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    total = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]

        hit_docs = [d for d in retrieved_docs if d in gt_docs]
        recall = len(hit_docs) / len(gt_docs) if gt_docs else 0.0
        total += recall

    mean_recall = total / n if n > 0 else 0.0
    print(f"ğŸ¯ Recall@{k}: {mean_recall:.4f}")
    return mean_recall

def precision_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    total = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]

        hit_docs = [d for d in retrieved_docs if d in gt_docs]
        precision = len(hit_docs) / k if k > 0 else 0.0
        total += precision

    mean_precision = total / n if n > 0 else 0.0
    print(f"ğŸ¯ Precision@{k}: {mean_precision:.4f}")
    return mean_precision

def f1_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    total_f1 = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]

        hit_docs = [d for d in retrieved_docs if d in gt_docs]
        precision = len(hit_docs) / k if k > 0 else 0.0
        recall = len(hit_docs) / len(gt_docs) if gt_docs else 0.0
        f1 = 0.0 if (precision + recall) == 0 else (2 * precision * recall) / (precision + recall)
        total_f1 += f1

    mean_f1 = total_f1 / n if n > 0 else 0.0
    print(f"ğŸ¯ F1@{k}: {mean_f1:.4f}")
    return mean_f1

def map_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    total_map = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]

        hits = 0
        ap = 0.0
        for i, doc in enumerate(retrieved_docs):
            if doc in gt_docs:
                hits += 1
                ap += hits / (i + 1)

        if hits > 0:
            ap /= hits
        total_map += ap

    mean_map = total_map / n if n > 0 else 0.0
    print(f"ğŸ¯ MAP@{k}: {mean_map:.4f}")
    return mean_map

def mrr_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    total_mrr = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]

        mrr = 0.0
        for i, doc in enumerate(retrieved_docs):
            if doc in gt_docs:
                mrr = 1.0 / (i + 1)
                break
        total_mrr += mrr

    mean_mrr = total_mrr / n if n > 0 else 0.0
    print(f"ğŸ¯ MRR@{k}: {mean_mrr:.4f}")
    return mean_mrr

def ndcg_k(df_clb, df_train, embedding, vector_db, k=5, nim_client: NIMClient = None):
    total_ndcg = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_docs = _parse_ground_truth_docs(row["Ground truth document"])

        search = get_cached_search_results(query, embedding, vector_db, k, nim_client)
        retrieved_docs = search["retrieved_docs"][:k]
        user_emb = search["user_embedding"]

        rels = []
        for doc_id in retrieved_docs:
            if doc_id in gt_docs:
                try:
                    doc_text = df_clb.loc[doc_id - 1, 'VÄƒn báº£n']
                    doc_emb = embedding.encode(doc_text)
                    s = cosine01(user_emb, doc_emb)
                    if s > 0.9: rels.append(3)
                    elif s > 0.7: rels.append(2)
                    elif s > 0.5: rels.append(1)
                    else: rels.append(0)
                except Exception as e:
                    rels.append(1)
            else:
                rels.append(0)

        ndcg_score = ndcg_at_k(rels, k)
        total_ndcg += ndcg_score

    mean_ndcg = total_ndcg / n if n > 0 else 0.0
    print(f"ğŸ¯ NDCG@{k}: {mean_ndcg:.4f}")
    return mean_ndcg

# LLM-judged retrieval context metrics
def context_precision_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None):
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for context_precision_k.")
    total_precision = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        search = get_cached_search_results(query, embedding, vector_db, k, nim)
        results = search["results"][:k]
        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. KhÃ´ng Ä‘Æ°á»£c thÃªm cÃ¢u dáº«n nhÆ° "Dá»±a trÃªn thÃ´ng tin tá»« ngá»¯ cáº£nh, dÆ°á»›i Ä‘Ã¢y lÃ ...", tráº£ lá»i trá»±c tiáº¿p vÃ o cÃ¢u há»i

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- Hiá»‡n táº¡i CLB ProPTIT cÃ³ 6 team dá»± Ã¡n: Team AI, Team Mobile, Team Data, Team Game, Team Web, Team Backend. CÃ¡c em sáº½ vÃ o team dá»± Ã¡n sau khi Ä‘Ã£ hoÃ n thÃ nh khÃ³a há»c Java.
- QuÃ¡ trÃ¬nh tuyá»ƒn thÃ nh viÃªn gá»“m ba vÃ²ng: Ä‘Æ¡n Ä‘Äƒng kÃ½, phá»ng váº¥n vÃ  thá»­ thÃ¡ch thá»±c táº¿. Trong vÃ²ng training, cÃ¡c anh chá»‹ sáº½ Ä‘Ã¡nh giÃ¡ dá»±a trÃªn thÃ¡i Ä‘á»™, tinh tháº§n há»c há»i vÃ  kháº£ nÄƒng lÃ m viá»‡c nhÃ³m cá»§a báº¡n.
- CLB lÃ  nÆ¡i giao lÆ°u, Ä‘Ã o táº¡o cÃ¡c mÃ´n láº­p trÃ¬nh vÃ  phÃ¡t triá»ƒn ká»¹ nÄƒng má»m. ThÃ nh viÃªn Ä‘Æ°á»£c tham gia cÃ¡c hoáº¡t Ä‘á»™ng há»c táº­p, dá»± Ã¡n, sá»± kiá»‡n vÃ  cÃ¡c chÆ°Æ¡ng trÃ¬nh giao lÆ°u vá»›i CLB khÃ¡c.
-Trong giai Ä‘oáº¡n training, CLB chÃº trá»ng Ä‘Ã¡nh giÃ¡ tinh tháº§n há»c há»i, kháº£ nÄƒng lÃ m viá»‡c nhÃ³m vÃ  sá»± tuÃ¢n thá»§ ná»™i quy. Viá»‡c hoÃ n thÃ nh tá»‘t nghÄ©a vá»¥ sáº½ giÃºp báº¡n ghi Ä‘iá»ƒm cao.
NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- LuÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man."""
            }
        ]
        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        reply = nim.chat(messages, temperature=0.0, max_tokens=512)

        batch_judge_messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t chuyÃªn gia AI Ä‘Ã¡nh giÃ¡ Context Precision trong há»‡ thá»‘ng RAG. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ Táº¤T Cáº¢ cÃ¡c ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p cÃ¹ng má»™t lÃºc.

QUY TRÃŒNH ÄÃNH GIÃ CHI TIáº¾T:
1. Äá»c ká»¹ cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
2. Äá»c ká»¹ cÃ¢u tráº£ lá»i tá»« mÃ´ hÃ¬nh AI
3. ÄÃ¡nh giÃ¡ tá»«ng ngá»¯ cáº£nh má»™t cÃ¡ch Ä‘á»™c láº­p
4. XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ há»— trá»£ cá»§a tá»«ng ngá»¯ cáº£nh cho cÃ¢u tráº£ lá»i

TIÃŠU CHÃ ÄÃNH GIÃ CHO Tá»ªNG NGá»® Cáº¢NH:
- ÄÃNH GIÃ = 1 náº¿u ngá»¯ cáº£nh CUNG Cáº¤P Äá»¦ THÃ”NG TIN hoáº·c Má»˜T PHáº¦N THÃ”NG TIN Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i
- ÄÃNH GIÃ = 0 náº¿u ngá»¯ cáº£nh KHÃ”NG CUNG Cáº¤P THÃ”NG TIN hoáº·c KHÃ”NG LIÃŠN QUAN Ä‘áº¿n cÃ¢u há»i
- Ngay cáº£ khi ngá»¯ cáº£nh chá»‰ cung cáº¥p má»™t pháº§n thÃ´ng tin há»¯u Ã­ch, váº«n Ä‘Ã¡nh giÃ¡ = 1
- Chá»‰ Ä‘Ã¡nh giÃ¡ = 0 khi ngá»¯ cáº£nh hoÃ n toÃ n khÃ´ng liÃªn quan hoáº·c khÃ´ng cÃ³ thÃ´ng tin gÃ¬ há»¯u Ã­ch

Äá»ŠNH Dáº NG OUTPUT Báº®T BUá»˜C:
Tráº£ vá» CHá»ˆ danh sÃ¡ch cÃ¡c sá»‘ 0 hoáº·c 1, ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y, KHÃ”NG cÃ³ text khÃ¡c:
VÃ­ dá»¥: 1,0,1,1,0

LÆ¯U Ã QUAN TRá»ŒNG:
- Sá»‘ lÆ°á»£ng káº¿t quáº£ PHáº¢I Báº°NG sá»‘ lÆ°á»£ng ngá»¯ cáº£nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡
- Thá»© tá»± káº¿t quáº£ PHáº¢I tÆ°Æ¡ng á»©ng vá»›i thá»© tá»± cÃ¡c ngá»¯ cáº£nh
- KHÃ”NG giáº£i thÃ­ch, KHÃ”NG thÃªm text nÃ o khÃ¡c ngoÃ i danh sÃ¡ch sá»‘"""
            }
        ]

        batch_content = f"CÃ‚U Há»I: {query}\n\nCÃ‚U TRáº¢ Lá»œI: {reply}\n\n"
        for i, r in enumerate(results):
            snippet = r.get("information", "")
            batch_content += f"NGá»® Cáº¢NH {i+1}:\n{snippet}\n\n"

        batch_judge_messages.append({
            "role": "user",
            "content": batch_content
        })

        batch_judged = nim.chat(batch_judge_messages, temperature=0.0, max_tokens=50).strip()

        try:
            import re
            numbers = re.findall(r'[01]', batch_judged)
            if len(numbers) == len(results):
                judged_results = [int(num) for num in numbers]
            else:
                judged_results = [1] * len(results)
        except Exception as e:
            judged_results = [1] * len(results)

        hits = sum(judged_results)
        precision = hits / k if k > 0 else 0
        total_precision += precision

    mean_precision = total_precision / n if n > 0 else 0.0
    print(f"Context Precision@{k}: {mean_precision:.3f}")
    return mean_precision

def context_recall_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None):
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for context_recall_k.")
    total_recall = 0.0
    n = len(df_train)

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_answer = row.get("Ground truth answer", "")
        search = get_cached_search_results(query, embedding, vector_db, k, nim)
        results = search["results"][:k]

        batch_judge_messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t chuyÃªn gia AI Ä‘Ã¡nh giÃ¡ Context Recall trong há»‡ thá»‘ng RAG. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ Táº¤T Cáº¢ cÃ¡c ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p cÃ¹ng má»™t lÃºc.

QUY TRÃŒNH ÄÃNH GIÃ CHI TIáº¾T:
1. Äá»c ká»¹ cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
2. Äá»c ká»¹ cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c (Ground Truth)
3. ÄÃ¡nh giÃ¡ tá»«ng ngá»¯ cáº£nh má»™t cÃ¡ch Ä‘á»™c láº­p
4. XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ há»— trá»£ cá»§a tá»«ng ngá»¯ cáº£nh cho cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c

TIÃŠU CHÃ ÄÃNH GIÃ CHO Tá»ªNG NGá»® Cáº¢NH:
- ÄÃNH GIÃ = 1 náº¿u ngá»¯ cáº£nh CUNG Cáº¤P Äá»¦ THÃ”NG TIN hoáº·c Má»˜T PHáº¦N THÃ”NG TIN Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i
- ÄÃNH GIÃ = 0 náº¿u ngá»¯ cáº£nh KHÃ”NG CUNG Cáº¤P THÃ”NG TIN hoáº·c KHÃ”NG LIÃŠN QUAN Ä‘áº¿n cÃ¢u há»i
- Ngay cáº£ khi ngá»¯ cáº£nh chá»‰ cung cáº¥p má»™t pháº§n thÃ´ng tin há»¯u Ã­ch, váº«n Ä‘Ã¡nh giÃ¡ = 1
- Chá»‰ Ä‘Ã¡nh giÃ¡ = 0 khi ngá»¯ cáº£nh hoÃ n toÃ n khÃ´ng liÃªn quan hoáº·c khÃ´ng cÃ³ thÃ´ng tin gÃ¬ há»¯u Ã­ch

Äá»ŠNH Dáº NG OUTPUT Báº®T BUá»˜C:
Tráº£ vá» CHá»ˆ danh sÃ¡ch cÃ¡c sá»‘ 0 hoáº·c 1, ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y, KHÃ”NG cÃ³ text khÃ¡c:
VÃ­ dá»¥: 1,0,1,1,0

LÆ¯U Ã QUAN TRá»ŒNG:
- Sá»‘ lÆ°á»£ng káº¿t quáº£ PHáº¢I Báº°NG sá»‘ lÆ°á»£ng ngá»¯ cáº£nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡
- Thá»© tá»± káº¿t quáº£ PHáº¢I tÆ°Æ¡ng á»©ng vá»›i thá»© tá»± cÃ¡c ngá»¯ cáº£nh
- KHÃ”NG giáº£i thÃ­ch, KHÃ”NG thÃªm text nÃ o khÃ¡c ngoÃ i danh sÃ¡ch sá»‘"""
            }
        ]
        batch_content = f"CÃ‚U Há»I: {query}\n\nCÃ‚U TRáº¢ Lá»œI CHÃNH XÃC: {gt_answer}\n\n"
        for i, r in enumerate(results):
            snippet = r.get("information", "")
            batch_content += f"NGá»® Cáº¢NH {i+1}:\n{snippet}\n\n"

        batch_judge_messages.append({
            "role": "user",
            "content": batch_content
        })

        batch_judged = nim.chat(batch_judge_messages, temperature=0.0, max_tokens=50).strip()

        try:
            import re
            numbers = re.findall(r'[01]', batch_judged)
            if len(numbers) == len(results):
                judged_results = [int(num) for num in numbers]
            else:
                judged_results = [1] * len(results)
        except Exception as e:
            judged_results = [1] * len(results)

        hits = sum(judged_results)
        recall = hits / k if k > 0 else 0
        total_recall += recall

    mean_recall = total_recall / n if n > 0 else 0.0
    print(f"Context Recall@{k}: {mean_recall:.3f}")
    return mean_recall

def context_entities_recall_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None):
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for context_entities_recall_k.")
    total_recall = 0.0
    n = len(df_train)
    print(f"ğŸ” Computing Context Entities Recall@{k} for {n} queries...")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        gt_answer = row.get("Ground truth answer", "")
        query = row["Query"]
        print(f"ğŸ” Query {idx+1:3d}/{n}: {query[:50]}...")

        search = get_cached_search_results(query, embedding, vector_db, k, nim)
        results = search["results"][:k]
        messages_entities = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn trÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« cÃ¢u tráº£ lá»i. Báº¡n sáº½ Ä‘Æ°á»£c cung cáº¥p má»™t cÃ¢u tráº£ lá»i vÃ  nhiá»‡m vá»¥ cá»§a báº¡n lÃ  trÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« cÃ¢u tráº£ lá»i Ä‘Ã³. CÃ¡c thá»±c thá»ƒ cÃ³ thá»ƒ lÃ  tÃªn ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, tá»• chá»©c, sá»± kiá»‡n, v.v. HÃ£y tráº£ lá»i dÆ°á»›i dáº¡ng má»™t danh sÃ¡ch cÃ¡c thá»±c thá»ƒ.
                VÃ­ dá»¥:
                CÃ¢u tráº£ lá»i: Náº¿u báº¡n thuá»™c ngÃ nh khÃ¡c báº¡n váº«n cÃ³ thá»ƒ tham gia CLB chÃºng mÃ¬nh. Náº¿u Ä‘á»‹nh hÆ°á»›ng cá»§a báº¡n hoÃ n toÃ n lÃ  theo CNTT thÃ¬ CLB cháº¯c cháº¯n lÃ  nÆ¡i phÃ¹ há»£p nháº¥t Ä‘á»ƒ cÃ¡c báº¡n phÃ¡t triá»ƒn. Trá»Ÿ ngáº¡i lá»›n nháº¥t sáº½ lÃ  do báº¡n theo má»™t hÆ°á»›ng khÃ¡c ná»¯a nÃªn sáº½ pháº£i táº­p trung vÃ o cáº£ 2 máº£ng nÃªn sáº½ cáº§n cá»‘ gáº¯ng nhiá»u hÆ¡n.
                ["ngÃ nh khÃ¡c", "CLB", "CNTT", "máº£ng]
                CÃ¢u tráº£ lá»i: CÃ¢u láº¡c bá»™ Láº­p TrÃ¬nh PTIT (Programming PTIT), tÃªn viáº¿t táº¯t lÃ  PROPTIT Ä‘Æ°á»£c thÃ nh láº­p ngÃ y 9/10/2011. Vá»›i phÆ°Æ¡ng chÃ¢m hoáº¡t Ä‘á»™ng "Chia sáº» Ä‘á»ƒ cÃ¹ng nhau phÃ¡t triá»ƒn", cÃ¢u láº¡c bá»™ lÃ  nÆ¡i giao lÆ°u, Ä‘Ã o táº¡o cÃ¡c mÃ´n láº­p trÃ¬nh vÃ  cÃ¡c mÃ´n há»c trong trÆ°á»ng, táº¡o Ä‘iá»u kiá»‡n Ä‘á»ƒ sinh viÃªn trong Há»c viá»‡n cÃ³ mÃ´i trÆ°á»ng há»c táº­p nÄƒng Ä‘á»™ng sÃ¡ng táº¡o. Slogan: Láº­p TrÃ¬nh PTIT - Láº­p trÃ¬nh tá»« trÃ¡i tim.
                ["CÃ¢u láº¡c bá»™ Láº­p TrÃ¬nh PTIT (Programming PTIT)", "PROPTIT", "9/10/2011", "Chia sáº» Ä‘á»ƒ cÃ¹ng nhau phÃ¡t triá»ƒn", "sinh viÃªn", "Há»c viá»‡n", "Láº­p TrÃ¬nh PTIT - Láº­p trÃ¬nh tá»« trÃ¡i tim"]"""
            },
            {
                "role": "user",
                "content": f"CÃ¢u tráº£ lá»i: {gt_answer}"
            }
        ]
        entities_text = nim.chat(messages_entities, temperature=0.0, max_tokens=256)

        # Parse entities
        entities = []
        try:
            start_idx = entities_text.find('[')
            end_idx = entities_text.rfind(']') + 1
            if start_idx != -1 and end_idx != -1:
                entities_str = entities_text[start_idx:end_idx]
                entities = eval(entities_str)
            else:
                entities = []
        except Exception as e:
            entities = []

        # Count presence of entities in the retrieved contexts
        tmp = len(entities)
        hits = 0
        if tmp == 0:
            recall = 0.0
        else:
            contexts = [r.get("information", "") for r in results]
            full_context = "\n".join(contexts)
            for entity in entities:
                if entity.strip() in full_context:
                    hits += 1
        recall = hits / tmp if tmp > 0 else 0.0
        total_recall += recall
        print(f"âœ… Context Entities Recall: {recall:.3f}")

    mean_recall = total_recall / n if n > 0 else 0.0
    print(f"[context_entities_recall@k] Mean: {mean_recall:.3f}")
    return mean_recall

# 10) LLM Answer Quality Metrics
def string_presence_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    print(f"ğŸ” [string_presence@k] Starting string_presence@k calculation with k={k}, hybrid={use_hybrid}, alpha={hybrid_alpha}")
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for string_presence_k.")
    total_presence = 0.0
    n = len(df_train)
    print(f"ğŸ“Š [string_presence@k] Processing {n} queries total")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_answer = row.get("Ground truth answer", "")

        print(f"ğŸ” [string_presence@k] Query {idx+1:3d}/{n}: '{query[:50]}...'")
        print(f"ğŸ“‹ [string_presence@k] GT Answer: '{gt_answer[:100]}...'")

        search = get_cached_search_results(query, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        results = search["results"][:k]

        print(f"ğŸ¯ [string_presence@k] Retrieved {len(results)} documents")

        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. Sá»­ dá»¥ng CÃC Tá»ª KHÃ“A vÃ  THá»°C THá»‚ chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c.
10. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- CLB luÃ´n khuyáº¿n khÃ­ch cÃ¡c báº¡n thuá»™c ngÃ nh khÃ¡c tham gia náº¿u cÃ³ Ä‘am mÃª.
- Trong quÃ¡ trÃ¬nh training, CLB Ä‘Ã¡nh giÃ¡ cao sá»± tiáº¿n bá»™ vÃ  tinh tháº§n há»c há»i cá»§a cÃ¡c thÃ nh viÃªn.
- ThÃ nh viÃªn cáº§n tuÃ¢n thá»§ ná»™i quy vá» trang phá»¥c, giá» giáº¥c vÃ  thÃ¡i Ä‘á»™ trong cÃ¡c buá»•i sinh hoáº¡t.
- CLB thÆ°á»ng xuyÃªn tá»• chá»©c cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u, há»£p tÃ¡c Ä‘á»ƒ má»Ÿ rá»™ng máº¡ng lÆ°á»›i vÃ  chia sáº» kinh nghiá»‡m.
- CLB thu tháº­p pháº£n há»“i Ä‘á»ƒ khÃ´ng ngá»«ng cáº£i thiá»‡n cháº¥t lÆ°á»£ng hoáº¡t Ä‘á»™ng.
- Khi tá»• chá»©c sá»± kiá»‡n, CLB chuáº©n bá»‹ ká»¹ lÆ°á»¡ng tá»« ná»™i dung Ä‘áº¿n háº­u cáº§n Ä‘á»ƒ Ä‘áº£m báº£o thÃ nh cÃ´ng.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man.
- Sá»­ dá»¥ng thÃ´ng tin chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao."""
            }
        ]
        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        response = nim.chat(messages, temperature=0.1, max_tokens=512)  # TÄƒng temperature vÃ  max_tokens Ä‘á»ƒ cáº£i thiá»‡n quality
        print(f"ğŸ¤– [string_presence@k] LLM Response: '{response[:100]}...'")

        # Extract entities from GT answer
        messages_entities = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn trÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« cÃ¢u tráº£ lá»i. Báº¡n sáº½ Ä‘Æ°á»£c cung cáº¥p má»™t cÃ¢u tráº£ lá»i vÃ  nhiá»‡m vá»¥ cá»§a báº¡n lÃ  trÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« cÃ¢u tráº£ lá»i Ä‘Ã³. CÃ¡c thá»±c thá»ƒ cÃ³ thá»ƒ lÃ  tÃªn ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, tá»• chá»©c, sá»± kiá»‡n, v.v. HÃ£y tráº£ lá»i dÆ°á»›i dáº¡ng má»™t danh sÃ¡ch cÃ¡c thá»±c thá»ƒ.
                VÃ­ dá»¥:
                CÃ¢u tráº£ lá»i: Náº¿u báº¡n thuá»™c ngÃ nh khÃ¡c báº¡n váº«n cÃ³ thá»ƒ tham gia CLB chÃºng mÃ¬nh. Náº¿u Ä‘á»‹nh hÆ°á»›ng cá»§a báº¡n hoÃ n toÃ n lÃ  theo CNTT thÃ¬ CLB cháº¯c cháº¯n lÃ  nÆ¡i phÃ¹ há»£p nháº¥t Ä‘á»ƒ cÃ¡c báº¡n phÃ¡t triá»ƒn. Trá»Ÿ ngáº¡i lá»›n nháº¥t sáº½ lÃ  do báº¡n theo má»™t hÆ°á»›ng khÃ¡c ná»¯a nÃªn sáº½ pháº£i táº­p trung vÃ o cáº£ 2 máº£ng nÃªn sáº½ cáº§n cá»‘ gáº¯ng nhiá»u hÆ¡n.
                ["ngÃ nh khÃ¡c", "CLB", "CNTT", "máº£ng]
                CÃ¢u tráº£ lá»i: CÃ¢u láº¡c bá»™ Láº­p TrÃ¬nh PTIT (Programming PTIT), tÃªn viáº¿t táº¯t lÃ  PROPTIT Ä‘Æ°á»£c thÃ nh láº­p ngÃ y 9/10/2011. Vá»›i phÆ°Æ¡ng chÃ¢m hoáº¡t Ä‘á»™ng "Chia sáº» Ä‘á»ƒ cÃ¹ng nhau phÃ¡t triá»ƒn", cÃ¢u láº¡c bá»™ lÃ  nÆ¡i giao lÆ°u, Ä‘Ã o táº¡o cÃ¡c mÃ´n láº­p trÃ¬nh vÃ  cÃ¡c mÃ´n há»c trong trÆ°á»ng, táº¡o Ä‘iá»u kiá»‡n Ä‘á»ƒ sinh viÃªn trong Há»c viá»‡n cÃ³ mÃ´i trÆ°á»ng há»c táº­p nÄƒng Ä‘á»™ng sÃ¡ng táº¡o. Slogan: Láº­p TrÃ¬nh PTIT - Láº­p trÃ¬nh tá»« trÃ¡i tim.
                ["CÃ¢u láº¡c bá»™ Láº­p TrÃ¬nh PTIT (Programming PTIT)", "PROPTIT", "9/10/2011", "Chia sáº» Ä‘á»ƒ cÃ¹ng nhau phÃ¡t triá»ƒn", "sinh viÃªn", "Há»c viá»‡n", "Láº­p TrÃ¬nh PTIT - Láº­p trÃ¬nh tá»« trÃ¡i tim"]"""
            },
            {
                "role": "user",
                "content": f"CÃ¢u tráº£ lá»i: {gt_answer}"
            }
        ]

        entities_text = nim.chat(messages_entities, temperature=0.1, max_tokens=512)
        entities = []
        try:
            start_idx = entities_text.find('[')
            end_idx = entities_text.rfind(']') + 1
            if start_idx != -1 and end_idx != -1:
                entities_str = entities_text[start_idx:end_idx]
                entities = eval(entities_str)
            else:
                entities = []
        except Exception as e:
            print(f"âš ï¸ [string_presence@k] Error parsing entities: {e}")
            entities = []

        print(f"ğŸ” [string_presence@k] Extracted entities: {entities}")

        # Count entity presence in response
        hits = 0
        for entity in entities:
            if entity.strip() in response:
                hits += 1
                print(f"âœ… [string_presence@k] Entity found: '{entity.strip()}'")

        presence = hits / len(entities) if len(entities) > 0 else 0
        print(f"ğŸ“ˆ [string_presence@k] Presence score: {presence:.4f} (Found: {hits}/{len(entities)})")

        total_presence += presence
        running_avg = total_presence / (idx + 1)
        print(f"ğŸ“Š [string_presence@k] Running average: {running_avg:.4f}")
        print()

    mean_presence = total_presence / n if n > 0 else 0.0
    print(f"ğŸ¯ [string_presence@k] FINAL RESULT: Mean presence = {mean_presence:.4f}")
    return mean_presence

def rouge_l_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    print(f"ğŸ” [rouge_l@k] Starting ROUGE-L@k calculation with k={k}, hybrid={use_hybrid}, alpha={hybrid_alpha}")
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for rouge_l_k.")
    rouge = Rouge()
    total_rouge_l = 0.0
    n = len(df_train)
    print(f"ğŸ“Š [rouge_l@k] Processing {n} queries total")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_answer = row.get("Ground truth answer", "")

        print(f"ğŸ” [rouge_l@k] Query {idx+1:3d}/{n}: '{query[:50]}...'")
        print(f"ğŸ“‹ [rouge_l@k] GT Answer: '{gt_answer[:100]}...'")

        search = get_cached_search_results(query, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        results = search["results"][:k]

        print(f"ğŸ¯ [rouge_l@k] Retrieved {len(results)} documents")

        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. Sá»­ dá»¥ng CÃC Tá»ª KHÃ“A vÃ  THá»°C THá»‚ chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c.
10. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- CLB luÃ´n khuyáº¿n khÃ­ch cÃ¡c báº¡n thuá»™c ngÃ nh khÃ¡c tham gia náº¿u cÃ³ Ä‘am mÃª.
- Trong quÃ¡ trÃ¬nh training, CLB Ä‘Ã¡nh giÃ¡ cao sá»± tiáº¿n bá»™ vÃ  tinh tháº§n há»c há»i cá»§a cÃ¡c thÃ nh viÃªn.
- ThÃ nh viÃªn cáº§n tuÃ¢n thá»§ ná»™i quy vá» trang phá»¥c, giá» giáº¥c vÃ  thÃ¡i Ä‘á»™ trong cÃ¡c buá»•i sinh hoáº¡t.
- CLB thÆ°á»ng xuyÃªn tá»• chá»©c cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u, há»£p tÃ¡c Ä‘á»ƒ má»Ÿ rá»™ng máº¡ng lÆ°á»›i vÃ  chia sáº» kinh nghiá»‡m.
- CLB thu tháº­p pháº£n há»“i Ä‘á»ƒ khÃ´ng ngá»«ng cáº£i thiá»‡n cháº¥t lÆ°á»£ng hoáº¡t Ä‘á»™ng.
- Khi tá»• chá»©c sá»± kiá»‡n, CLB chuáº©n bá»‹ ká»¹ lÆ°á»¡ng tá»« ná»™i dung Ä‘áº¿n háº­u cáº§n Ä‘á»ƒ Ä‘áº£m báº£o thÃ nh cÃ´ng.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man.
- Sá»­ dá»¥ng thÃ´ng tin chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao."""
            }
        ]
        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        response = nim.chat(messages, temperature=0.1, max_tokens=512)  # TÄƒng temperature vÃ  max_tokens Ä‘á»ƒ cáº£i thiá»‡n quality
        print(f"ğŸ¤– [rouge_l@k] LLM Response: '{response[:100]}...'")

        # Calculate ROUGE-L
        scores = rouge.get_scores(response, gt_answer)
        rouge_l = scores[0]['rouge-l']['f']
        print(f"ğŸ“Š [rouge_l@k] ROUGE-L calculation:")
        print(f"   - Response length: {len(response)} chars")
        print(f"   - GT Answer length: {len(gt_answer)} chars")
        print(f"   - ROUGE-L F1: {rouge_l:.4f}")

        total_rouge_l += rouge_l
        running_avg = total_rouge_l / (idx + 1)
        print(f"ğŸ“Š [rouge_l@k] Running average: {running_avg:.4f}")
        print()

    mean_rouge_l = total_rouge_l / n if n > 0 else 0.0
    print(f"ğŸ¯ [rouge_l@k] FINAL RESULT: Mean ROUGE-L = {mean_rouge_l:.4f}")
    return mean_rouge_l

def bleu_4_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    print(f"ğŸ” [bleu_4@k] Starting BLEU-4@k calculation with k={k}, hybrid={use_hybrid}, alpha={hybrid_alpha}")
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for bleu_4_k.")
    total_bleu_4 = 0.0
    n = len(df_train)
    smoothing_function = SmoothingFunction().method1
    print(f"ğŸ“Š [bleu_4@k] Processing {n} queries total")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_answer = row.get("Ground truth answer", "")

        print(f"ğŸ” [bleu_4@k] Query {idx+1:3d}/{n}: '{query[:50]}...'")
        print(f"ğŸ“‹ [bleu_4@k] GT Answer: '{gt_answer[:100]}...'")

        search = get_cached_search_results(query, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        results = search["results"][:k]

        print(f"ğŸ¯ [bleu_4@k] Retrieved {len(results)} documents")

        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. Sá»­ dá»¥ng CÃC Tá»ª KHÃ“A vÃ  THá»°C THá»‚ chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c.
10. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- CLB luÃ´n khuyáº¿n khÃ­ch cÃ¡c báº¡n thuá»™c ngÃ nh khÃ¡c tham gia náº¿u cÃ³ Ä‘am mÃª.
- Trong quÃ¡ trÃ¬nh training, CLB Ä‘Ã¡nh giÃ¡ cao sá»± tiáº¿n bá»™ vÃ  tinh tháº§n há»c há»i cá»§a cÃ¡c thÃ nh viÃªn.
- ThÃ nh viÃªn cáº§n tuÃ¢n thá»§ ná»™i quy vá» trang phá»¥c, giá» giáº¥c vÃ  thÃ¡i Ä‘á»™ trong cÃ¡c buá»•i sinh hoáº¡t.
- CLB thÆ°á»ng xuyÃªn tá»• chá»©c cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u, há»£p tÃ¡c Ä‘á»ƒ má»Ÿ rá»™ng máº¡ng lÆ°á»›i vÃ  chia sáº» kinh nghiá»‡m.
- CLB thu tháº­p pháº£n há»“i Ä‘á»ƒ khÃ´ng ngá»«ng cáº£i thiá»‡n cháº¥t lÆ°á»£ng hoáº¡t Ä‘á»™ng.
- Khi tá»• chá»©c sá»± kiá»‡n, CLB chuáº©n bá»‹ ká»¹ lÆ°á»¡ng tá»« ná»™i dung Ä‘áº¿n háº­u cáº§n Ä‘á»ƒ Ä‘áº£m báº£o thÃ nh cÃ´ng.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man.
- Sá»­ dá»¥ng thÃ´ng tin chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao."""
            }
        ]
        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        response = nim.chat(messages, temperature=0.1, max_tokens=512)  # TÄƒng temperature vÃ  max_tokens Ä‘á»ƒ cáº£i thiá»‡n quality
        print(f"ğŸ¤– [bleu_4@k] LLM Response: '{response[:100]}...'")

        # Calculate BLEU-4
        reference = [gt_answer.split()]
        candidate = response.split()
        bleu_score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)

        print(f"ğŸ“Š [bleu_4@k] BLEU-4 calculation:")
        print(f"   - Reference tokens: {len(reference[0])}")
        print(f"   - Candidate tokens: {len(candidate)}")
        print(f"   - BLEU-4 score: {bleu_score:.4f}")

        total_bleu_4 += bleu_score
        running_avg = total_bleu_4 / (idx + 1)
        print(f"ğŸ“Š [bleu_4@k] Running average: {running_avg:.4f}")
        print()

    mean_bleu_4 = total_bleu_4 / n if n > 0 else 0.0
    print(f"ğŸ¯ [bleu_4@k] FINAL RESULT: Mean BLEU-4 = {mean_bleu_4:.4f}")
    return mean_bleu_4

def groundedness_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    print(f"ğŸ” [groundedness@k] Starting groundedness@k calculation with k={k}, hybrid={use_hybrid}, alpha={hybrid_alpha}")
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for groundedness_k.")
    total_groundedness = 0.0
    n = len(df_train)
    print(f"ğŸ“Š [groundedness@k] Processing {n} queries total")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_answer = row.get("Ground truth answer", "")

        print(f"ğŸ” [groundedness@k] Query {idx+1:3d}/{n}: '{query[:50]}...'")
        print(f"ğŸ“‹ [groundedness@k] GT Answer: '{gt_answer[:100]}...'")

        search = get_cached_search_results(query, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        results = search["results"][:k]

        print(f"ğŸ¯ [groundedness@k] Retrieved {len(results)} documents")
        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. Sá»­ dá»¥ng CÃC Tá»ª KHÃ“A vÃ  THá»°C THá»‚ chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c.
10. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- CLB luÃ´n khuyáº¿n khÃ­ch cÃ¡c báº¡n thuá»™c ngÃ nh khÃ¡c tham gia náº¿u cÃ³ Ä‘am mÃª.
- Trong quÃ¡ trÃ¬nh training, CLB Ä‘Ã¡nh giÃ¡ cao sá»± tiáº¿n bá»™ vÃ  tinh tháº§n há»c há»i cá»§a cÃ¡c thÃ nh viÃªn.
- ThÃ nh viÃªn cáº§n tuÃ¢n thá»§ ná»™i quy vá» trang phá»¥c, giá» giáº¥c vÃ  thÃ¡i Ä‘á»™ trong cÃ¡c buá»•i sinh hoáº¡t.
- CLB thÆ°á»ng xuyÃªn tá»• chá»©c cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u, há»£p tÃ¡c Ä‘á»ƒ má»Ÿ rá»™ng máº¡ng lÆ°á»›i vÃ  chia sáº» kinh nghiá»‡m.
- CLB thu tháº­p pháº£n há»“i Ä‘á»ƒ khÃ´ng ngá»«ng cáº£i thiá»‡n cháº¥t lÆ°á»£ng hoáº¡t Ä‘á»™ng.
- Khi tá»• chá»©c sá»± kiá»‡n, CLB chuáº©n bá»‹ ká»¹ lÆ°á»¡ng tá»« ná»™i dung Ä‘áº¿n háº­u cáº§n Ä‘á»ƒ Ä‘áº£m báº£o thÃ nh cÃ´ng.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man.
- Sá»­ dá»¥ng thÃ´ng tin chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao."""
            }
        ]
        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        response = nim.chat(messages, temperature=0.1, max_tokens=512)  # TÄƒng temperature vÃ  max_tokens Ä‘á»ƒ cáº£i thiá»‡n quality
        print(f"ğŸ¤– [groundedness@k] LLM Response: '{response[:100]}...'")

        # Split response into sentences
        sentences = response.split('. ')
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            print(f"âš ï¸ [groundedness@k] No sentences found, skipping...")
            continue

        print(f"ğŸ“ [groundedness@k] Split into {len(sentences)} sentences for evaluation")

        batch_judge_messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t chuyÃªn gia Ä‘Ã¡nh giÃ¡ Groundedness trong há»‡ thá»‘ng RAG. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ Táº¤T Cáº¢ cÃ¡c cÃ¢u Ä‘Æ°á»£c cung cáº¥p cÃ¹ng má»™t lÃºc.

QUY TRÃŒNH ÄÃNH GIÃ CHI TIáº¾T:
1. Äá»c ká»¹ cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
2. Äá»c ká»¹ ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p
3. ÄÃ¡nh giÃ¡ tá»«ng cÃ¢u má»™t cÃ¡ch Ä‘á»™c láº­p
4. PhÃ¢n loáº¡i má»©c Ä‘á»™ groundedness cá»§a tá»«ng cÃ¢u

TIÃŠU CHÃ ÄÃNH GIÃ CHO Tá»ªNG CÃ‚U:
- supported: Ná»™i dung cÃ¢u Ä‘Æ°á»£c ngá»¯ cáº£nh há»— trá»£ hoáº·c suy ra trá»±c tiáº¿p
- unsupported: Ná»™i dung cÃ¢u khÃ´ng Ä‘Æ°á»£c ngá»¯ cáº£nh há»— trá»£, vÃ  khÃ´ng thá»ƒ suy ra tá»« Ä‘Ã³
- contradictory: Ná»™i dung cÃ¢u trÃ¡i ngÆ°á»£c hoáº·c mÃ¢u thuáº«n vá»›i ngá»¯ cáº£nh
- no_rad: CÃ¢u khÃ´ng yÃªu cáº§u kiá»ƒm tra thá»±c táº¿ (vÃ­ dá»¥: cÃ¢u chÃ o há»i, Ã½ kiáº¿n cÃ¡ nhÃ¢n, cÃ¢u há»i tu tá»«, disclaimers)

Äá»ŠNH Dáº NG OUTPUT Báº®T BUá»˜C:
Tráº£ vá» CHá»ˆ danh sÃ¡ch cÃ¡c nhÃ£n, ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y, KHÃ”NG cÃ³ text khÃ¡c:
VÃ­ dá»¥: supported,unsupported,supported,no_rad,contradictory

LÆ¯U Ã QUAN TRá»ŒNG:
- Sá»‘ lÆ°á»£ng káº¿t quáº£ PHáº¢I Báº°NG sá»‘ lÆ°á»£ng cÃ¢u Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡
- Thá»© tá»± káº¿t quáº£ PHáº¢I tÆ°Æ¡ng á»©ng vá»›i thá»© tá»± cÃ¡c cÃ¢u
- KHÃ”NG giáº£i thÃ­ch, KHÃ”NG thÃªm text nÃ o khÃ¡c ngoÃ i danh sÃ¡ch nhÃ£n"""
            }
        ]
        batch_content = f"CÃ‚U Há»I: {query}\n\nNGá»® Cáº¢NH:\n{context}\n\n"
        for i, sentence in enumerate(sentences):
            batch_content += f"CÃ‚U {i+1}: {sentence}\n"

        batch_judge_messages.append({
            "role": "user",
            "content": batch_content
        })

        
        batch_judged = nim.chat(batch_judge_messages, temperature=0.0, max_tokens=100).strip()
        print(f"ğŸ” [groundedness@k] Batch judgment result: '{batch_judged}'")

        
        try:
            # Extract labels from response
            import re
            labels = re.findall(r'\b(supported|unsupported|contradictory|no_rad)\b', batch_judged.lower())
            if len(labels) == len(sentences):
                judged_results = labels
            else:
                # Fallback: assume all are supported if parsing fails
                print(f"âš ï¸ [groundedness@k] Failed to parse batch results, using fallback (all supported)")
                judged_results = ["supported"] * len(sentences)
        except Exception as e:
            print(f"âš ï¸ [groundedness@k] Error parsing batch results: {e}, using fallback (all supported)")
            judged_results = ["supported"] * len(sentences)

        hits = sum(1 for label in judged_results if label == "supported")
        cnt = len(sentences)
        groundedness_score = hits / cnt if cnt > 0 else 0.0

        print(f"ğŸ“Š [groundedness@k] Groundedness evaluation:")
        print(f"   - Total sentences: {cnt}")
        print(f"   - Supported sentences: {hits}")
        print(f"   - Groundedness score: {groundedness_score:.4f}")

        total_groundedness += groundedness_score
        running_avg = total_groundedness / (idx + 1)
        print(f"ğŸ“Š [groundedness@k] Running average: {running_avg:.4f}")
        print()

    mean_groundedness = total_groundedness / n if n > 0 else 0.0
    print(f"ğŸ¯ [groundedness@k] FINAL RESULT: Mean groundedness = {mean_groundedness:.4f}")
    return mean_groundedness

def response_relevancy_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    print(f"ğŸ” [response_relevancy@k] Starting response_relevancy@k calculation with k={k}, hybrid={use_hybrid}, alpha={hybrid_alpha}")
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for response_relevancy_k.")
    total_relevancy = 0.0
    n = len(df_train)
    print(f"ğŸ“Š [response_relevancy@k] Processing {n} queries total")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]

        print(f"ğŸ” [response_relevancy@k] Query {idx+1:3d}/{n}: '{query[:50]}...'")

        search = get_cached_search_results(query, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        results = search["results"][:k]

        print(f"ğŸ¯ [response_relevancy@k] Retrieved {len(results)} documents")

        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. Sá»­ dá»¥ng CÃC Tá»ª KHÃ“A vÃ  THá»°C THá»‚ chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c.
10. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- CLB luÃ´n khuyáº¿n khÃ­ch cÃ¡c báº¡n thuá»™c ngÃ nh khÃ¡c tham gia náº¿u cÃ³ Ä‘am mÃª.
- Trong quÃ¡ trÃ¬nh training, CLB Ä‘Ã¡nh giÃ¡ cao sá»± tiáº¿n bá»™ vÃ  tinh tháº§n há»c há»i cá»§a cÃ¡c thÃ nh viÃªn.
- ThÃ nh viÃªn cáº§n tuÃ¢n thá»§ ná»™i quy vá» trang phá»¥c, giá» giáº¥c vÃ  thÃ¡i Ä‘á»™ trong cÃ¡c buá»•i sinh hoáº¡t.
- CLB thÆ°á»ng xuyÃªn tá»• chá»©c cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u, há»£p tÃ¡c Ä‘á»ƒ má»Ÿ rá»™ng máº¡ng lÆ°á»›i vÃ  chia sáº» kinh nghiá»‡m.
- CLB thu tháº­p pháº£n há»“i Ä‘á»ƒ khÃ´ng ngá»«ng cáº£i thiá»‡n cháº¥t lÆ°á»£ng hoáº¡t Ä‘á»™ng.
- Khi tá»• chá»©c sá»± kiá»‡n, CLB chuáº©n bá»‹ ká»¹ lÆ°á»¡ng tá»« ná»™i dung Ä‘áº¿n háº­u cáº§n Ä‘á»ƒ Ä‘áº£m báº£o thÃ nh cÃ´ng.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man.
- Sá»­ dá»¥ng thÃ´ng tin chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao."""
            }
        ]
        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        response = nim.chat(messages, temperature=0.1, max_tokens=512)  # TÄƒng temperature vÃ  max_tokens Ä‘á»ƒ cáº£i thiá»‡n quality
        print(f"ğŸ¤– [response_relevancy@k] LLM Response: '{response[:100]}...'")

        # Generate related questions from the response
        messages_related = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn táº¡o ra cÃ¡c cÃ¢u há»i liÃªn quan tá»« má»™t cÃ¢u tráº£ lá»i. Báº¡n sáº½ Ä‘Æ°á»£c cung cáº¥p má»™t cÃ¢u tráº£ lá»i vÃ  nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o ra cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n cÃ¢u tráº£ lá»i Ä‘Ã³. HÃ£y táº¡o ra Ã­t nháº¥t 5 cÃ¢u há»i liÃªn quan, má»—i cÃ¢u há»i nÃªn ngáº¯n gá»n vÃ  rÃµ rÃ ng. Tráº£ lá»i dÆ°á»›i dáº¡ng list cÃ¡c cÃ¢u há»i nhÆ° á»Ÿ vÃ­ dá»¥ dÆ°á»›i. LÆ¯U Ã: Tráº£ lá»i dÆ°á»›i dáº¡ng ["cÃ¢u há»i 1", "cÃ¢u há»i 2", "cÃ¢u há»i 3", ...], bao gá»“m cáº£ dáº¥u ngoáº·c vuÃ´ng.
    VÃ­ dá»¥:
    CÃ¢u tráº£ lá»i: CÃ¢u láº¡c bá»™ Láº­p TrÃ¬nh PTIT (Programming PTIT), tÃªn viáº¿t táº¯t lÃ  PROPTIT Ä‘Æ°á»£c thÃ nh láº­p ngÃ y 9/10/2011. Vá»›i phÆ°Æ¡ng chÃ¢m hoáº¡t Ä‘á»™ng "Chia sáº» Ä‘á»ƒ cÃ¹ng nhau phÃ¡t triá»ƒn", cÃ¢u láº¡c bá»™ lÃ  nÆ¡i giao lÆ°u, Ä‘Ã o táº¡o cÃ¡c mÃ´n láº­p trÃ¬nh vÃ  cÃ¡c mÃ´n há»c trong trÆ°á»ng, táº¡o Ä‘iá»u kiá»‡n Ä‘á»ƒ sinh viÃªn trong Há»c viá»‡n cÃ³ mÃ´i trÆ°á»ng há»c táº­p nÄƒng Ä‘á»™ng sÃ¡ng táº¡o. Slogan: Láº­p TrÃ¬nh PTIT - Láº­p trÃ¬nh tá»« trÃ¡i tim.
    Output cá»§a báº¡n: "["CLB Láº­p TrÃ¬nh PTIT Ä‘Æ°á»£c thÃ nh láº­p khi nÃ o?", "Slogan cá»§a CLB lÃ  gÃ¬?", "Má»¥c tiÃªu cá»§a CLB lÃ  gÃ¬?"]"
    CÃ¢u tráº£ lá»i: Náº¿u báº¡n thuá»™c ngÃ nh khÃ¡c báº¡n váº«n cÃ³ thá»ƒ tham gia CLB chÃºng mÃ¬nh. Náº¿u Ä‘á»‹nh hÆ°á»›ng cá»§a báº¡n hoÃ n toÃ n lÃ  theo CNTT thÃ¬ CLB cháº¯c cháº¯n lÃ  nÆ¡i phÃ¹ há»£p nháº¥t Ä‘á»ƒ cÃ¡c báº¡n phÃ¡t triá»ƒn. Trá»Ÿ ngáº¡i lá»›n nháº¥t sáº½ lÃ  do báº¡n theo má»™t hÆ°á»›ng khÃ¡c ná»¯a nÃªn sáº½ pháº£i táº­p trung vÃ o cáº£ 2 máº£ng nÃªn sáº½ cáº§n cá»‘ gáº¯ng nhiá»u hÆ¡n.
    Output cá»§a báº¡n: "["NgÃ nh nÃ o cÃ³ thá»ƒ tham gia CLB?", "CLB phÃ¹ há»£p vá»›i nhá»¯ng ai?", "Trá»Ÿ ngáº¡i lá»›n nháº¥t khi tham gia CLB lÃ  gÃ¬?"]"""
            },
            {
                "role": "user",
                "content": f"CÃ¢u tráº£ lá»i: {response}"
            }
        ]

        related_questions_text = nim.chat(messages_related, temperature=0.0, max_tokens=256)
        print(f"ğŸ” [response_relevancy@k] Related questions response: '{related_questions_text[:100]}...'")

        # Parse related questions
        try:
            start_idx = related_questions_text.find('[')
            end_idx = related_questions_text.rfind(']') + 1
            if start_idx != -1 and end_idx != -1:
                questions_str = related_questions_text[start_idx:end_idx]
                related_questions = eval(questions_str)
            else:
                related_questions = []
        except Exception as e:
            print(f"âš ï¸ [response_relevancy@k] Error parsing related questions: {e}")
            related_questions = []

        print(f"ğŸ“ [response_relevancy@k] Generated {len(related_questions)} related questions:")
        for i, q in enumerate(related_questions):
            print(f"   {i+1}. {q}")

        # Evaluate if original query can answer the related questions using batch processing
        if not related_questions:
            print(f"âš ï¸ [response_relevancy@k] No related questions found, skipping...")
            continue

        batch_judge_messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t chuyÃªn gia AI Ä‘Ã¡nh giÃ¡ Response Relevancy. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ Táº¤T Cáº¢ cÃ¡c cÃ¢u há»i liÃªn quan cÃ¹ng má»™t lÃºc.

QUY TRÃŒNH ÄÃNH GIÃ CHI TIáº¾T:
1. Äá»c ká»¹ cÃ¢u tráº£ lá»i gá»‘c Ä‘Æ°á»£c cung cáº¥p
2. ÄÃ¡nh giÃ¡ tá»«ng cÃ¢u há»i liÃªn quan má»™t cÃ¡ch Ä‘á»™c láº­p
3. XÃ¡c Ä‘á»‹nh xem cÃ¢u tráº£ lá»i gá»‘c cÃ³ thá»ƒ tráº£ lá»i Ä‘Æ°á»£c cÃ¢u há»i Ä‘Ã³ hay khÃ´ng

TIÃŠU CHÃ ÄÃNH GIÃ CHO Tá»ªNG CÃ‚U Há»I:
- ÄÃNH GIÃ = 1 náº¿u cÃ¢u tráº£ lá»i gá»‘c CÃ“ THá»‚ tráº£ lá»i Ä‘Æ°á»£c cÃ¢u há»i liÃªn quan
- ÄÃNH GIÃ = 0 náº¿u cÃ¢u tráº£ lá»i gá»‘c KHÃ”NG THá»‚ tráº£ lá»i Ä‘Æ°á»£c cÃ¢u há»i liÃªn quan
- CÃ¢u tráº£ lá»i cÃ³ thá»ƒ tráº£ lá»i trá»±c tiáº¿p hoáº·c suy luáº­n logic tá»« ná»™i dung

Äá»ŠNH Dáº NG OUTPUT Báº®T BUá»˜C:
Tráº£ vá» CHá»ˆ danh sÃ¡ch cÃ¡c sá»‘ 0 hoáº·c 1, ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y, KHÃ”NG cÃ³ text khÃ¡c:
VÃ­ dá»¥: 1,0,1,1,0

LÆ¯U Ã QUAN TRá»ŒNG:
- Sá»‘ lÆ°á»£ng káº¿t quáº£ PHáº¢I Báº°NG sá»‘ lÆ°á»£ng cÃ¢u há»i liÃªn quan Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡
- Thá»© tá»± káº¿t quáº£ PHáº¢I tÆ°Æ¡ng á»©ng vá»›i thá»© tá»± cÃ¡c cÃ¢u há»i
- KHÃ”NG giáº£i thÃ­ch, KHÃ”NG thÃªm text nÃ o khÃ¡c ngoÃ i danh sÃ¡ch sá»‘"""
            }
        ]
        batch_content = f"CÃ‚U TRáº¢ Lá»œI Gá»C: {response}\n\n"
        for i, q in enumerate(related_questions):
            batch_content += f"CÃ‚U Há»I LIÃŠN QUAN {i+1}: {q}\n"

        batch_judge_messages.append({
            "role": "user",
            "content": batch_content
        })

        batch_judged = nim.chat(batch_judge_messages, temperature=0.0, max_tokens=50).strip()
        print(f"ğŸ” [response_relevancy@k] Batch judgment result: '{batch_judged}'")

        
        try:
            # Extract numbers from response
            import re
            numbers = re.findall(r'[01]', batch_judged)
            if len(numbers) == len(related_questions):
                judged_results = [int(num) for num in numbers]
            else:
                # Fallback: assume all are answerable if parsing fails
                print(f"âš ï¸ [response_relevancy@k] Failed to parse batch results, using fallback (all 1s)")
                judged_results = [1] * len(related_questions)
        except Exception as e:
            print(f"âš ï¸ [response_relevancy@k] Error parsing batch results: {e}, using fallback (all 1s)")
            judged_results = [1] * len(related_questions)

        hits = sum(judged_results)
        relevancy_score = hits / len(related_questions) if len(related_questions) > 0 else 0.0

        print(f"ğŸ“Š [response_relevancy@k] Relevancy evaluation:")
        print(f"   - Total related questions: {len(related_questions)}")
        print(f"   - Answerable questions: {hits}")
        print(f"   - Relevancy score: {relevancy_score:.4f}")

        total_relevancy += relevancy_score
        running_avg = total_relevancy / (idx + 1)
        print(f"ğŸ“Š [response_relevancy@k] Running average: {running_avg:.4f}")
        print()

    mean_relevancy = total_relevancy / n if n > 0 else 0.0
    print(f"ğŸ¯ [response_relevancy@k] FINAL RESULT: Mean relevancy = {mean_relevancy:.4f}")
    return mean_relevancy

def noise_sensitivity_k(df_clb, df_train, embedding, vector_db, k=5, nim: NIMClient = None, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    print(f"ğŸ” [noise_sensitivity@k] Starting noise_sensitivity@k calculation with k={k}, hybrid={use_hybrid}, alpha={hybrid_alpha}")
    if nim is None:
        raise RuntimeError("âŒ NIM client is required for noise_sensitivity_k.")
    total_sensitivity = 0.0
    n = len(df_train)
    print(f"ğŸ“Š [noise_sensitivity@k] Processing {n} queries total")

    for idx, row in enumerate(df_train.iterrows()):
        row = row[1]
        query = row["Query"]
        gt_answer = row.get("Ground truth answer", "")

        print(f"ğŸ” [noise_sensitivity@k] Query {idx+1:3d}/{n}: '{query[:50]}...'")
        print(f"ğŸ“‹ [noise_sensitivity@k] GT Answer: '{gt_answer[:100]}...'")

        search = get_cached_search_results(query, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        results = search["results"][:k]

        print(f"ğŸ¯ [noise_sensitivity@k] Retrieved {len(results)} documents")

        context = "Content tá»« cÃ¡c tÃ i liá»‡u liÃªn quan:\n"
        context += "\n".join([result["information"] for result in results])

        messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

NGUYÃŠN Táº®C TRáº¢ Lá»œI Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin ngoÃ i context.
2. Tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC, vÃ  TRá»°C TIáº¾P vÃ o cÃ¢u há»i.
3. KHÃ”NG Ä‘Æ°á»£c thÃªm lá»i chÃ o há»i, cáº£m Æ¡n, hoáº·c cÃ¢u xÃ£ giao khÃ´ng cáº§n thiáº¿t.
4. KHÃ”NG Ä‘Æ°á»£c nÃ³i "Xin lá»—i", "TÃ´i khÃ´ng biáº¿t", "KhÃ´ng cÃ³ thÃ´ng tin" - PHáº¢I tráº£ lá»i dá»±a trÃªn context cÃ³ sáºµn.
5. Náº¿u context khÃ´ng Ä‘á»§, hÃ£y suy luáº­n LOGIC tá»« thÃ´ng tin cÃ³ sáºµn mÃ  KHÃ”NG bá»‹a thÃªm.
6. Táº­p trung tráº£ lá»i CÃ‚U Há»I CHÃNH, bá» qua thÃ´ng tin khÃ´ng liÃªn quan.
7. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i phong cÃ¡ch tráº£ lá»i cá»§a con ngÆ°á»i.
8. Æ¯u tiÃªn xÆ°ng lÃ  "CLB" khi nÃ³i vá» tá»• chá»©c.
9. Sá»­ dá»¥ng CÃC Tá»ª KHÃ“A vÃ  THá»°C THá»‚ chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c.
10. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, trá»« khi cÃ¢u há»i yÃªu cáº§u khÃ¡c.

VÃ­ dá»¥ tham kháº£o vá» phong cÃ¡ch tráº£ lá»i:
- CLB luÃ´n khuyáº¿n khÃ­ch cÃ¡c báº¡n thuá»™c ngÃ nh khÃ¡c tham gia náº¿u cÃ³ Ä‘am mÃª.
- Trong quÃ¡ trÃ¬nh training, CLB Ä‘Ã¡nh giÃ¡ cao sá»± tiáº¿n bá»™ vÃ  tinh tháº§n há»c há»i cá»§a cÃ¡c thÃ nh viÃªn.
- ThÃ nh viÃªn cáº§n tuÃ¢n thá»§ ná»™i quy vá» trang phá»¥c, giá» giáº¥c vÃ  thÃ¡i Ä‘á»™ trong cÃ¡c buá»•i sinh hoáº¡t.
- CLB thÆ°á»ng xuyÃªn tá»• chá»©c cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u, há»£p tÃ¡c Ä‘á»ƒ má»Ÿ rá»™ng máº¡ng lÆ°á»›i vÃ  chia sáº» kinh nghiá»‡m.
- CLB thu tháº­p pháº£n há»“i Ä‘á»ƒ khÃ´ng ngá»«ng cáº£i thiá»‡n cháº¥t lÆ°á»£ng hoáº¡t Ä‘á»™ng.
- Khi tá»• chá»©c sá»± kiá»‡n, CLB chuáº©n bá»‹ ká»¹ lÆ°á»¡ng tá»« ná»™i dung Ä‘áº¿n háº­u cáº§n Ä‘á»ƒ Ä‘áº£m báº£o thÃ nh cÃ´ng.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT.
- Giá»¯ cÃ¢u tráº£ lá»i trong pháº¡m vi 2-4 cÃ¢u, khÃ´ng lan man.
- Sá»­ dá»¥ng thÃ´ng tin chÃ­nh xÃ¡c tá»« context Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao."""
            }
        ]
        messages.append({
            "role": "user",
            "content": context + "\n\nCÃ¢u há»i: " + query
        })

        response = nim.chat(messages, temperature=0.1, max_tokens=512)  # TÄƒng temperature vÃ  max_tokens Ä‘á»ƒ cáº£i thiá»‡n quality
        print(f"ğŸ¤– [noise_sensitivity@k] LLM Response: '{response[:100]}...'")

        # Split response into sentences and evaluate each using batch processing
        sentences = response.split('. ')
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            print(f"âš ï¸ [noise_sensitivity@k] No sentences found, skipping...")
            continue

        print(f"ğŸ“ [noise_sensitivity@k] Split into {len(sentences)} sentences for evaluation")

        batch_judge_messages = [
            {
                "role": "system",
                "content": """Báº¡n lÃ  má»™t chuyÃªn gia Ä‘Ã¡nh giÃ¡ Noise Sensitivity trong há»‡ thá»‘ng RAG. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ Táº¤T Cáº¢ cÃ¡c cÃ¢u Ä‘Æ°á»£c cung cáº¥p cÃ¹ng má»™t lÃºc.

QUY TRÃŒNH ÄÃNH GIÃ CHI TIáº¾T:
1. Äá»c ká»¹ cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
2. Äá»c ká»¹ ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p
3. ÄÃ¡nh giÃ¡ tá»«ng cÃ¢u má»™t cÃ¡ch Ä‘á»™c láº­p
4. XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ nháº¡y cáº£m cá»§a tá»«ng cÃ¢u vá»›i noise

TIÃŠU CHÃ ÄÃNH GIÃ CHO Tá»ªNG CÃ‚U:
- ÄÃNH GIÃ = 1 náº¿u ná»™i dung cÃ¢u Ä‘Æ°á»£c ngá»¯ cáº£nh há»— trá»£ hoáº·c suy ra trá»±c tiáº¿p
- ÄÃNH GIÃ = 0 náº¿u ná»™i dung cÃ¢u khÃ´ng Ä‘Æ°á»£c ngá»¯ cáº£nh há»— trá»£, vÃ  khÃ´ng thá»ƒ suy ra tá»« Ä‘Ã³

Äá»ŠNH Dáº NG OUTPUT Báº®T BUá»˜C:
Tráº£ vá» CHá»ˆ danh sÃ¡ch cÃ¡c sá»‘ 0 hoáº·c 1, ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y, KHÃ”NG cÃ³ text khÃ¡c:
VÃ­ dá»¥: 1,0,1,1,0

LÆ¯U Ã QUAN TRá»ŒNG:
- Sá»‘ lÆ°á»£ng káº¿t quáº£ PHáº¢I Báº°NG sá»‘ lÆ°á»£ng cÃ¢u Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡
- Thá»© tá»± káº¿t quáº£ PHáº¢I tÆ°Æ¡ng á»©ng vá»›i thá»© tá»± cÃ¡c cÃ¢u
- KHÃ”NG giáº£i thÃ­ch, KHÃ”NG thÃªm text nÃ o khÃ¡c ngoÃ i danh sÃ¡ch sá»‘"""
            }
        ]
        batch_content = f"CÃ‚U Há»I: {query}\n\nNGá»® Cáº¢NH:\n{context}\n\n"
        for i, sentence in enumerate(sentences):
            batch_content += f"CÃ‚U {i+1}: {sentence}\n"

        batch_judge_messages.append({
            "role": "user",
            "content": batch_content
        })

        
        batch_judged = nim.chat(batch_judge_messages, temperature=0.1, max_tokens=512).strip()
        print(f"ğŸ” [noise_sensitivity@k] Batch judgment result: '{batch_judged}'")

        
        try:
            # Extract numbers from response
            import re
            numbers = re.findall(r'[01]', batch_judged)
            if len(numbers) == len(sentences):
                judged_results = [int(num) for num in numbers]
            else:
                # Fallback: assume all are supported if parsing fails
                print(f"âš ï¸ [noise_sensitivity@k] Failed to parse batch results, using fallback (all 1s)")
                judged_results = [1] * len(sentences)
        except Exception as e:
            print(f"âš ï¸ [noise_sensitivity@k] Error parsing batch results: {e}, using fallback (all 1s)")
            judged_results = [1] * len(sentences)

        hits = sum(1 for result in judged_results if result == 0)  # Count unsupported sentences
        sensitivity_score = hits / len(sentences) if len(sentences) > 0 else 0.0

        print(f"ğŸ“Š [noise_sensitivity@k] Sensitivity evaluation:")
        print(f"   - Total sentences: {len(sentences)}")
        print(f"   - Unsupported sentences: {hits}")
        print(f"   - Sensitivity score: {sensitivity_score:.4f}")

        total_sensitivity += sensitivity_score
        running_avg = total_sensitivity / (idx + 1)
        print(f"ğŸ“Š [noise_sensitivity@k] Running average: {running_avg:.4f}")
        print()

    mean_sensitivity = total_sensitivity / n if n > 0 else 0.0
    print(f"ğŸ¯ [noise_sensitivity@k] FINAL RESULT: Mean sensitivity = {mean_sensitivity:.4f}")
    return mean_sensitivity

# Calculate all retrieval metrics 
def calculate_metrics_retrieval(df_clb, df_train, embedding, vector_db, train: bool, nim: NIMClient, compute_llm_metrics: bool = True, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    """
    Returns a DataFrame with rows for k in {3,5,7} and metrics columns as BTC's file.
    """
    clear_search_cache()

    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "hit@k": [],
        "recall@k": [],
        "precision@k": [],
        "f1@k": [],
        "map@k": [],
        "mrr@k": [],
        "ndcg@k": [],
        "context_precision@k": [],
        "context_recall@k": [],
        "context_entities_recall@k": []
    }

    for k in k_values:
        print(f"\n================  Metrics @K={k}  ================")
        metrics["K"].append(k)
        h = hit_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        r = recall_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        p = precision_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        f1 = f1_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        mp = map_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        mrr = mrr_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)
        nd = ndcg_k(df_clb, df_train, embedding, vector_db, k, nim, use_hybrid, hybrid_alpha)

        # Compute LLM-based metrics only if requested
        if compute_llm_metrics:
            cp = context_precision_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
            cr = context_recall_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
            cer = context_entities_recall_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
        else:
            print("â­ï¸ Skipping LLM-based metrics (context_precision@k, context_recall@k, context_entities_recall@k)")
            cp = 0.0
            cr = 0.0
            cer = 0.0

        metrics["hit@k"].append(round(h, 2))
        metrics["recall@k"].append(round(r, 2))
        metrics["precision@k"].append(round(p, 2))
        metrics["f1@k"].append(round(f1, 2))
        metrics["map@k"].append(round(mp, 2))
        metrics["mrr@k"].append(round(mrr, 2))
        metrics["ndcg@k"].append(round(nd, 2))
        metrics["context_precision@k"].append(round(cp, 2))
        metrics["context_recall@k"].append(round(cr, 2))
        metrics["context_entities_recall@k"].append(round(cer, 2))

    df = pd.DataFrame(metrics)
    out = "metrics_retrieval_train.csv" if train else "metrics_retrieval_test.csv"
    df.to_csv(out, index=False)
    print(f"\nğŸ’¾ Saved retrieval metrics to {out}")
    return df

# Calculate all LLM answer metrics
def calculate_metrics_llm_answer(df_clb, df_train, embedding, vector_db, train: bool, nim: NIMClient, compute_llm_answer_metrics: bool = True, use_hybrid: bool = True, hybrid_alpha: float = 0.7):
    """
    Returns a DataFrame with rows for k in {3,5,7} and LLM answer metrics columns.
    """
    if not compute_llm_answer_metrics:
        print("â­ï¸ Skipping LLM answer metrics computation")
        return pd.DataFrame()
    
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "string_presence@k": [],
        "rouge_l@k": [],
        "bleu_4@k": [],
        "groundedness@k": [],
        "response_relevancy@k": [],
        "noise_sensitivity@k": []
    }

    for k in k_values:
        print(f"\n================  LLM Answer Metrics @K={k}  ================")
        metrics["K"].append(k)
        sp = string_presence_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
        rl = rouge_l_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
        b4 = bleu_4_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
        gr = groundedness_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
        rr = response_relevancy_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)
        ns = noise_sensitivity_k(df_clb, df_train, embedding, vector_db, k, nim=nim, use_hybrid=use_hybrid, hybrid_alpha=hybrid_alpha)

        metrics["string_presence@k"].append(round(sp, 2))
        metrics["rouge_l@k"].append(round(rl, 2))
        metrics["bleu_4@k"].append(round(b4, 2))
        metrics["groundedness@k"].append(round(gr, 2))
        metrics["response_relevancy@k"].append(round(rr, 2))
        metrics["noise_sensitivity@k"].append(round(ns, 2))

    df = pd.DataFrame(metrics)
    out = "metrics_llm_answer_train.csv" if train else "metrics_llm_answer_test.csv"
    df.to_csv(out, index=False)
    print(f"\nğŸ’¾ Saved LLM answer metrics to {out}")
    return df

def main():
    print("ğŸš€ NeoRAG Cup 2025 â€” Local Pipeline with LLM Reranker\n")

    USE_TRAIN_DATA = False
    COMPUTE_LLM_METRICS = True
    COMPUTE_LLM_ANSWER_METRICS = True

    print("ğŸ“‹ Loading environment variables...")
    nim_api_key = os.environ.get("NIM_API_KEY", "")
    nim_base_url = os.environ.get("NIM_BASE_URL", "https://integrate.api.nvidia.com/v1")
    mongodb_uri = os.environ.get("MONGODB_URI", "")

    if not nim_api_key:
        raise RuntimeError("âŒ NIM_API_KEY not found in .env file")
    if not mongodb_uri:
        raise RuntimeError("âŒ MONGODB_URI not found in .env file")

    print("âœ… Environment variables loaded.")

    if not os.path.exists("CLB_PROPTIT.csv"):
        raise FileNotFoundError("âŒ CLB_PROPTIT.csv not found.")

    if USE_TRAIN_DATA:
        data_file = "train_data_proptit.xlsx"
        is_train = True
        print(f"ğŸ“‹ Using {data_file} - TRAIN MODE")
    else:
        data_file = "test_data_proptit.xlsx"
        is_train = False
        print(f"ğŸ“‹ Using {data_file} - TEST MODE")

    if not os.path.exists(data_file):
        raise FileNotFoundError(f"âŒ {data_file} not found.")

    df_clb = pd.read_csv("CLB_PROPTIT.csv")
    df_train = pd.read_excel(data_file)

    print(f"ğŸ“„ Loaded: {len(df_clb)} CLB docs, {len(df_train)} queries.")

    embedding = Embeddings()
    vector_db = VectorDatabaseAtlas(mongodb_uri)

    col_name = "information"
    try:
        count_existing = vector_db.count_documents(col_name)
    except Exception:
        count_existing = 0

    if count_existing == 0:
        print("ğŸ§  Building document embeddings...")
        cnt = 0
        for i, row in df_clb.iterrows():
            text = str(row.get("VÄƒn báº£n", "")).strip()
            if not text:
                continue
            emb = embedding.encode(text)
            cnt += 1
            vector_db.insert_document(col_name, {
                "title": f"Document {cnt}",
                "information": text,
                "embedding": emb.tolist()
            })
        print(f"âœ… Stored {cnt} documents.")
    else:
        print(f"âœ… Found {count_existing} existing documents.")

    nim = NIMClient(api_key=nim_api_key, base_url=nim_base_url, model="meta/llama-3.1-405b-instruct")

    print("\nğŸ” Computing retrieval metrics...")
    df_metrics = calculate_metrics_retrieval(df_clb, df_train, embedding, vector_db, train=is_train, nim=nim, compute_llm_metrics=COMPUTE_LLM_METRICS)
    print("\nğŸ“Š Retrieval Metrics:\n", df_metrics)

    print("\nğŸ¤– Computing LLM answer quality metrics...")
    df_llm_answer_metrics = calculate_metrics_llm_answer(df_clb, df_train, embedding, vector_db, train=is_train, nim=nim, compute_llm_answer_metrics=COMPUTE_LLM_ANSWER_METRICS)
    if not df_llm_answer_metrics.empty:
        print("\nğŸ“Š LLM Answer Metrics:\n", df_llm_answer_metrics)

if __name__ == "__main__":
    main()
